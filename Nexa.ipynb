{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z7rLXpJV6wU0"
      },
      "outputs": [],
      "source": [
        "!fusermount -u /content/drive 2>/dev/null || true\n",
        "!rm -rf /content/drive\n",
        "!mkdir -p /content/drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkHtS-1t6bpC",
        "outputId": "b4978897-2cb0-4200-b4c9-d092ed1f9a76"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "VIDEO_EXTS = {\".mp4\", \".mov\", \".mkv\", \".avi\"}\n",
        "\n",
        "video_paths = [p for p in DATA_ROOT.rglob(\"*\") if p.is_file() and p.suffix.lower() in VIDEO_EXTS]\n",
        "\n",
        "rows = []\n",
        "for vp in sorted(video_paths):\n",
        "    rows.append({\n",
        "        \"video_path\": str(vp),\n",
        "        \"label_raw\": vp.name,\n",
        "        \"label\": extract_label_from_filename(vp.name)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"Total videos:\", len(df))\n",
        "\n",
        "if len(df) == 0:\n",
        "    print(\"No videos found under:\", DATA_ROOT)\n",
        "    print(\"Check whether videos are in another folder or Drive path is different.\")\n",
        "else:\n",
        "    print(\"\\nTop labels:\\n\", df[\"label\"].value_counts().head(20))\n",
        "    out_csv = DATA_ROOT / \"labels.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"\\nSaved:\", out_csv)\n"
      ],
      "metadata": {
        "id": "ysMNcXf48Ps3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b4e95f-2155-4219-e5e5-57c16df326b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos: 77\n",
            "\n",
            "Top labels:\n",
            " label\n",
            "mma_kick              4\n",
            "pointing              3\n",
            "jumping               2\n",
            "jogging               2\n",
            "reacting              2\n",
            "push                  2\n",
            "punching              2\n",
            "crying                2\n",
            "angry                 2\n",
            "sitting               2\n",
            "talking               2\n",
            "running               2\n",
            "standing_thumbs_up    2\n",
            "shaking_hands         2\n",
            "stand_up              2\n",
            "sad_idle              2\n",
            "leaning_on_a_wall     2\n",
            "leaning               1\n",
            "laughing              1\n",
            "pointing_(1)          1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Saved: /content/drive/MyDrive/synthetic_videos/labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/drive/MyDrive/synthetic_videos | head -n 50\n",
        "!find /content/drive/MyDrive/synthetic_videos -type f \\( -iname \"*.mp4\" -o -iname \"*.mov\" -o -iname \"*.mkv\" -o -iname \"*.avi\" \\) | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qaocbp1khQm",
        "outputId": "07019c21-1943-4754-fa95-d8a67544aabb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 32M\n",
            "-rw------- 1 root root 178K Sep 27 14:21 air_squat0001-0112.mp4\n",
            "-rw------- 1 root root 1.1M Sep 27 15:21 Angry0001-0899.mp4\n",
            "-rw------- 1 root root 1.4M Sep 27 14:12 Angry0001-1151.mp4\n",
            "-rw------- 1 root root 197K Sep 27 15:22 Boxing0001-0126.mp4\n",
            "-rw------- 1 root root 8.5K Sep 27 14:13 Ch08_nonPBR0001-0002.mp4\n",
            "-rw------- 1 root root 203K Sep 27 15:24 Cheering0001-0088.mp4\n",
            "-rw------- 1 root root  84K Sep 27 15:24 Clapping0001-0070.mp4\n",
            "-rw------- 1 root root 442K Sep 27 14:16 Crying0001-0189.mp4\n",
            "-rw------- 1 root root 504K Sep 27 15:30 Crying0001-0376.mp4\n",
            "-rw------- 1 root root 1.6M Sep 27 15:38 dancing_twerk0001-0912.mp4\n",
            "-rw------- 1 root root 226K Sep 27 16:17 edge_slip0001-0148.mp4\n",
            "-rw------- 1 root root 440K Sep 27 16:11 Excited0001-0198.mp4\n",
            "-rw------- 1 root root 310K Sep 27 16:20 falling_from_losing_balance0001-0401.mp4\n",
            "drwx------ 5 root root 4.0K Jan 14 17:49 frames\n",
            "-rw------- 1 root root 411K Sep 27 16:22 hip_hop_dancing0001-0202.mp4\n",
            "-rw------- 1 root root 277K Sep 27 14:17 Jogging0001-0078.mp4\n",
            "-rw------- 1 root root 270K Sep 27 16:13 Jogging0001-0154.mp4\n",
            "-rw------- 1 root root 210K Sep 27 14:18 Jumping0001-0114.mp4\n",
            "-rw------- 1 root root 164K Sep 27 16:14 Jumping0001-0158.mp4\n",
            "-rw------- 1 root root 8.2K Jan 14 17:49 labels_clean.csv\n",
            "-rw------- 1 root root 7.7K Jan 14 17:52 labels.csv\n",
            "-rw------- 1 root root 743K Sep 27 15:47 Laughing0001-0589.mp4\n",
            "-rw------- 1 root root 180K Sep 27 14:20 Leaning0001-0201.mp4\n",
            "-rw------- 1 root root 137K Sep 27 13:50 leaning_on_a_wall0001-0097.mp4\n",
            "-rw------- 1 root root 150K Sep 27 15:50 leaning_on_a_wall0001-0116.mp4\n",
            "-rw------- 1 root root 1.1M Sep 27 15:57 look_around0001-0801.mp4\n",
            "-rw------- 1 root root 442K Sep 27 13:08 male_batch2__Mma Kick0001-0267.mp4\n",
            "-rw------- 1 root root 359K Sep 27 13:11 male_batch2__Pointing0001-0267.mp4\n",
            "-rw------- 1 root root 353K Sep 27 13:14 male_batch2__Punching0001-0267.mp4\n",
            "-rw------- 1 root root 503K Sep 27 13:17 male_batch2__Push0001-0267.mp4\n",
            "-rw------- 1 root root 315K Sep 27 13:20 male_batch2__Reacting0001-0267.mp4\n",
            "-rw------- 1 root root 243K Sep 27 13:51 mma_kick0001-0097.mp4\n",
            "-rw------- 1 root root 243K Sep 27 13:43 Mma Kick0001-0097.mp4\n",
            "-rw------- 1 root root 228K Sep 27 15:58 mma_kick0001-0101.mp4\n",
            "-rw------- 1 root root 682K Sep 27 13:26 Mma Kick0001-0267.mp4\n",
            "-rw------- 1 root root 455K Sep 27 16:01 nervously_look_around0001-0377.mp4\n",
            "-rw------- 1 root root 574K Sep 27 16:04 northern_soul_spin0001-0243.mp4\n",
            "-rw------- 1 root root 153K Sep 27 13:44 Pointing0001-0097.mp4\n",
            "-rw------- 1 root root 252K Sep 27 16:16 Pointing0001-0216.mp4\n",
            "-rw------- 1 root root 546K Sep 27 13:30 Pointing0001-0267.mp4\n",
            "-rw------- 1 root root 230K Sep 27 15:49 Pointing (1)0001-0207.mp4\n",
            "-rw------- 1 root root 163K Sep 27 13:52 pull_heavy_object0001-0097.mp4\n",
            "-rw------- 1 root root 1.3M Sep 27 16:32 pulling_a_rope0001-0884.mp4\n",
            "-rw------- 1 root root 108K Sep 27 16:35 Punching0001-0063.mp4\n",
            "-rw------- 1 root root 200K Sep 27 13:45 Punching0001-0097.mp4\n",
            "-rw------- 1 root root 173K Sep 27 13:47 Push0001-0097.mp4\n",
            "-rw------- 1 root root 693K Sep 27 16:39 Push0001-0481.mp4\n",
            "-rw------- 1 root root 163K Sep 27 16:51 quick_formal_bow0001-0165.mp4\n",
            "-rw------- 1 root root 141K Sep 27 13:48 Reacting0001-0097.mp4\n",
            "77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pp9FXApin6cy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "labels_path = DATA_ROOT / \"labels.csv\"\n",
        "\n",
        "df = pd.read_csv(labels_path)\n",
        "\n",
        "def clean_label(lbl: str) -> str:\n",
        "    lbl = str(lbl).lower().strip()\n",
        "    lbl = lbl.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
        "    # remove windows copy suffix like _(1), (1), _1\n",
        "    lbl = re.sub(r\"\\(\\d+\\)$\", \"\", lbl)\n",
        "    lbl = re.sub(r\"_\\(\\d+\\)$\", \"\", lbl)\n",
        "    lbl = re.sub(r\"_\\d+$\", \"\", lbl)\n",
        "    lbl = re.sub(r\"__+\", \"_\", lbl)\n",
        "    lbl = re.sub(r\"_+$\", \"\", lbl)\n",
        "    return lbl\n",
        "\n",
        "df[\"label\"] = df[\"label\"].apply(clean_label)\n",
        "\n",
        "# Show final class counts\n",
        "counts = df[\"label\"].value_counts()\n",
        "print(\"Total videos:\", len(df))\n",
        "print(\"Total classes:\", df[\"label\"].nunique())\n",
        "print(\"\\nClass counts:\\n\", counts)\n",
        "\n",
        "# If any class has only 1 sample, stratified split may fail.\n",
        "min_count = counts.min()\n",
        "print(\"\\nMin samples in a class:\", min_count)\n",
        "\n",
        "# Create split\n",
        "if min_count >= 2:\n",
        "    # First split train vs temp\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df, test_size=0.30, random_state=42, stratify=df[\"label\"]\n",
        "    )\n",
        "    # Split temp into val and test\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df, test_size=0.50, random_state=42, stratify=temp_df[\"label\"]\n",
        "    )\n",
        "else:\n",
        "    # Fallback: no stratify if some class has only 1 sample\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42)\n",
        "\n",
        "df[\"split\"] = \"train\"\n",
        "df.loc[val_df.index, \"split\"] = \"val\"\n",
        "df.loc[test_df.index, \"split\"] = \"test\"\n",
        "\n",
        "# Save\n",
        "clean_path = DATA_ROOT / \"labels_clean.csv\"\n",
        "split_path = DATA_ROOT / \"split.csv\"\n",
        "df.to_csv(clean_path, index=False)\n",
        "df.to_csv(split_path, index=False)\n",
        "\n",
        "print(\"\\nSaved:\", clean_path)\n",
        "print(\"Saved:\", split_path)\n",
        "\n",
        "print(\"\\nSplit counts:\\n\", df[\"split\"].value_counts())\n",
        "print(\"\\nExample rows:\\n\", df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dAHj4VvnQG1",
        "outputId": "dbf44fc1-32b7-47ed-9dd3-53534f319dc1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos: 77\n",
            "Total classes: 54\n",
            "\n",
            "Class counts:\n",
            " label\n",
            "mma_kick                       4\n",
            "pointing                       4\n",
            "sitting                        3\n",
            "jogging                        2\n",
            "angry                          2\n",
            "crying                         2\n",
            "talking                        2\n",
            "reacting                       2\n",
            "push                           2\n",
            "jumping                        2\n",
            "punching                       2\n",
            "running                        2\n",
            "leaning_on_a_wall              2\n",
            "waving                         2\n",
            "shaking_hands                  2\n",
            "stand_up                       2\n",
            "sad_idle                       2\n",
            "standing_thumbs_up             2\n",
            "clapping                       1\n",
            "cheering                       1\n",
            "ch08_nonpbr                    1\n",
            "boxing                         1\n",
            "t_pose                         1\n",
            "surprised                      1\n",
            "remy                           1\n",
            "leaning                        1\n",
            "laughing                       1\n",
            "excited                        1\n",
            "yelling                        1\n",
            "walking                        1\n",
            "hip_hop_dancing                1\n",
            "look_around                    1\n",
            "male_batch2_mma_kick           1\n",
            "male_batch2_pointing           1\n",
            "dancing_twerk                  1\n",
            "edge_slip                      1\n",
            "falling_from_losing_balance    1\n",
            "air_squat                      1\n",
            "nervously_look_around          1\n",
            "male_batch2_reacting           1\n",
            "male_batch2_push               1\n",
            "male_batch2_punching           1\n",
            "quick_formal_bow               1\n",
            "pulling_a_rope                 1\n",
            "pull_heavy_object              1\n",
            "northern_soul_spin             1\n",
            "sitting_talking                1\n",
            "standing_w_briefcase_idle      1\n",
            "start_walking                  1\n",
            "talking_on_phone               1\n",
            "tut_hip_hop_dance              1\n",
            "walking_backwards              1\n",
            "waving(1_hand)                 1\n",
            "waving_(2_hands)               1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Min samples in a class: 1\n",
            "\n",
            "Saved: /content/drive/MyDrive/synthetic_videos/labels_clean.csv\n",
            "Saved: /content/drive/MyDrive/synthetic_videos/split.csv\n",
            "\n",
            "Split counts:\n",
            " split\n",
            "train    53\n",
            "val      12\n",
            "test     12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Example rows:\n",
            "                                           video_path  \\\n",
            "0  /content/drive/MyDrive/synthetic_videos/Angry0...   \n",
            "1  /content/drive/MyDrive/synthetic_videos/Angry0...   \n",
            "2  /content/drive/MyDrive/synthetic_videos/Boxing...   \n",
            "3  /content/drive/MyDrive/synthetic_videos/Ch08_n...   \n",
            "4  /content/drive/MyDrive/synthetic_videos/Cheeri...   \n",
            "\n",
            "                  label_raw        label  split  \n",
            "0        Angry0001-0899.mp4        angry    val  \n",
            "1        Angry0001-1151.mp4        angry  train  \n",
            "2       Boxing0001-0126.mp4       boxing  train  \n",
            "3  Ch08_nonPBR0001-0002.mp4  ch08_nonpbr  train  \n",
            "4     Cheering0001-0088.mp4     cheering   test  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "df = pd.read_csv(DATA_ROOT / \"split.csv\")\n",
        "\n",
        "# Map raw labels -> grouped labels\n",
        "GROUP_MAP = {\n",
        "    # Emotions\n",
        "    \"angry\": \"emotion\",\n",
        "    \"crying\": \"emotion\",\n",
        "    \"laughing\": \"emotion\",\n",
        "    \"excited\": \"emotion\",\n",
        "    \"yelling\": \"emotion\",\n",
        "    \"surprised\": \"emotion\",\n",
        "    \"sad_idle\": \"emotion\",\n",
        "\n",
        "    # Communication / social\n",
        "    \"talking\": \"social\",\n",
        "    \"sitting_talking\": \"social\",\n",
        "    \"talking_on_phone\": \"social\",\n",
        "    \"shaking_hands\": \"social\",\n",
        "    \"pointing\": \"social\",\n",
        "    \"clapping\": \"social\",\n",
        "    \"cheering\": \"social\",\n",
        "    \"waving\": \"social\",\n",
        "    \"waving(1_hand)\": \"social\",\n",
        "    \"waving_(2_hands)\": \"social\",\n",
        "\n",
        "    # Locomotion\n",
        "    \"walking\": \"locomotion\",\n",
        "    \"start_walking\": \"locomotion\",\n",
        "    \"walking_backwards\": \"locomotion\",\n",
        "    \"running\": \"locomotion\",\n",
        "    \"jogging\": \"locomotion\",\n",
        "\n",
        "    # Physical / sport actions\n",
        "    \"punching\": \"physical\",\n",
        "    \"boxing\": \"physical\",\n",
        "    \"mma_kick\": \"physical\",\n",
        "    \"push\": \"physical\",\n",
        "    \"pulling_a_rope\": \"physical\",\n",
        "    \"pull_heavy_object\": \"physical\",\n",
        "\n",
        "    # Pose / idle / misc\n",
        "    \"sitting\": \"pose_idle\",\n",
        "    \"standing_thumbs_up\": \"pose_idle\",\n",
        "    \"standing_w_briefcase_idle\": \"pose_idle\",\n",
        "    \"stand_up\": \"pose_idle\",\n",
        "    \"leaning\": \"pose_idle\",\n",
        "    \"leaning_on_a_wall\": \"pose_idle\",\n",
        "    \"look_around\": \"pose_idle\",\n",
        "    \"nervously_look_around\": \"pose_idle\",\n",
        "    \"t_pose\": \"pose_idle\",\n",
        "    \"air_squat\": \"pose_idle\",\n",
        "    \"falling_from_losing_balance\": \"pose_idle\",\n",
        "    \"edge_slip\": \"pose_idle\",\n",
        "    \"ch08_nonpbr\": \"pose_idle\",\n",
        "    \"remy\": \"pose_idle\",\n",
        "    \"hip_hop_dancing\": \"pose_idle\",\n",
        "    \"tut_hip_hop_dance\": \"pose_idle\",\n",
        "    \"dancing_twerk\": \"pose_idle\",\n",
        "    \"northern_soul_spin\": \"pose_idle\",\n",
        "    \"quick_formal_bow\": \"pose_idle\",\n",
        "}\n",
        "\n",
        "def assign_group(lbl: str) -> str:\n",
        "    lbl = str(lbl)\n",
        "    # remove male_batch2_ prefix to map those into same group\n",
        "    if lbl.startswith(\"male_batch2_\"):\n",
        "        lbl2 = lbl.replace(\"male_batch2_\", \"\")\n",
        "    else:\n",
        "        lbl2 = lbl\n",
        "    return GROUP_MAP.get(lbl2, \"pose_idle\")  # default\n",
        "\n",
        "df[\"group_label\"] = df[\"label\"].apply(assign_group)\n",
        "\n",
        "print(\"Grouped class counts:\\n\", df[\"group_label\"].value_counts())\n",
        "print(\"\\nGrouped class counts by split:\\n\", pd.crosstab(df[\"split\"], df[\"group_label\"]))\n",
        "\n",
        "out_path = DATA_ROOT / \"split_grouped.csv\"\n",
        "df.to_csv(out_path, index=False)\n",
        "print(\"\\nSaved:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEVW74IaoCCA",
        "outputId": "c157c02a-4098-445e-99b6-741817da620b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grouped class counts:\n",
            " group_label\n",
            "pose_idle     29\n",
            "social        17\n",
            "physical      14\n",
            "emotion       10\n",
            "locomotion     7\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Grouped class counts by split:\n",
            " group_label  emotion  locomotion  physical  pose_idle  social\n",
            "split                                                        \n",
            "test               1           4         0          3       4\n",
            "train              7           3        11         21      11\n",
            "val                2           0         3          5       2\n",
            "\n",
            "Saved: /content/drive/MyDrive/synthetic_videos/split_grouped.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "df = pd.read_csv(DATA_ROOT / \"split_grouped.csv\")\n",
        "\n",
        "# Merge locomotion into physical\n",
        "df[\"group_label\"] = df[\"group_label\"].replace({\"locomotion\": \"physical\"})\n",
        "\n",
        "print(\"New group counts:\\n\", df[\"group_label\"].value_counts())\n",
        "print(\"\\nNew grouped counts by split:\\n\", pd.crosstab(df[\"split\"], df[\"group_label\"]))\n",
        "\n",
        "out_path = DATA_ROOT / \"split_grouped_final.csv\"\n",
        "df.to_csv(out_path, index=False)\n",
        "print(\"\\nSaved:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-cTv4jbo3yW",
        "outputId": "b79a687c-8394-4495-bc4c-898a7a237f3c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New group counts:\n",
            " group_label\n",
            "pose_idle    29\n",
            "physical     21\n",
            "social       17\n",
            "emotion      10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "New grouped counts by split:\n",
            " group_label  emotion  physical  pose_idle  social\n",
            "split                                            \n",
            "test               1         4          3       4\n",
            "train              7        14         21      11\n",
            "val                2         3          5       2\n",
            "\n",
            "Saved: /content/drive/MyDrive/synthetic_videos/split_grouped_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "FRAMES_ROOT = DATA_ROOT / \"frames\"\n",
        "CSV_PATH = DATA_ROOT / \"split_grouped_final.csv\"\n",
        "\n",
        "NUM_FRAMES = 16\n",
        "IMG_SIZE = 224\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "def extract_frames(video_path, out_dir, num_frames=16):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total <= 0:\n",
        "        cap.release()\n",
        "        return False\n",
        "\n",
        "    idxs = np.linspace(0, total - 1, num_frames).astype(int)\n",
        "    saved = 0\n",
        "\n",
        "    for i, idx in enumerate(idxs):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "        cv2.imwrite(str(out_dir / f\"frame_{i:02d}.jpg\"), frame)\n",
        "        saved += 1\n",
        "\n",
        "    cap.release()\n",
        "    return saved == num_frames\n",
        "\n",
        "# Create folders and extract\n",
        "for _, row in df.iterrows():\n",
        "    split = row[\"split\"]\n",
        "    label = row[\"group_label\"]\n",
        "    video_path = Path(row[\"video_path\"])\n",
        "\n",
        "    out_dir = FRAMES_ROOT / split / label / video_path.stem\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ok = extract_frames(video_path, out_dir, NUM_FRAMES)\n",
        "    if not ok:\n",
        "        print(\"Warning: incomplete frames for\", video_path)\n",
        "\n",
        "print(\"Frame extraction complete.\")\n"
      ],
      "metadata": {
        "id": "2WBjk98jp7-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0503187e-fe2b-416b-a4a0-145418bdfca2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive/synthetic_videos/frames -type f | wc -l\n"
      ],
      "metadata": {
        "id": "jFm4_Mxrqg9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e4f7db-aba6-4c70-8bc1-b6d7ddb82464"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "FRAMES_ROOT = DATA_ROOT / \"frames\"\n",
        "CSV_PATH = DATA_ROOT / \"split_grouped_final.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Fixed class order (important for consistency)\n",
        "CLASS_NAMES = [\"emotion\", \"social\", \"physical\", \"pose_idle\"]\n",
        "class_to_idx = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
        "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
        "\n",
        "print(\"Class mapping:\", class_to_idx)\n",
        "\n",
        "# Basic transform (keep simple for baseline)\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),  # [0,1]\n",
        "])\n",
        "\n",
        "class VideoFramesDataset(Dataset):\n",
        "    def __init__(self, df, split: str):\n",
        "        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        video_path = Path(row[\"video_path\"])\n",
        "        label = row[\"group_label\"]\n",
        "        y = class_to_idx[label]\n",
        "\n",
        "        # Folder where frames were saved\n",
        "        frames_dir = FRAMES_ROOT / self.split / label / video_path.stem\n",
        "\n",
        "        # Load 16 frames in sorted order\n",
        "        frame_files = sorted(frames_dir.glob(\"frame_*.jpg\"))\n",
        "        if len(frame_files) != 16:\n",
        "            raise RuntimeError(f\"Expected 16 frames, got {len(frame_files)} for {frames_dir}\")\n",
        "\n",
        "        frames = []\n",
        "        for ff in frame_files:\n",
        "            img = Image.open(ff).convert(\"RGB\")\n",
        "            frames.append(img_tf(img))\n",
        "\n",
        "        # (T, C, H, W)\n",
        "        x = torch.stack(frames, dim=0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "train_ds = VideoFramesDataset(df, \"train\")\n",
        "val_ds   = VideoFramesDataset(df, \"val\")\n",
        "test_ds  = VideoFramesDataset(df, \"test\")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Quick sanity check\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"Batch x shape:\", xb.shape)   # expected: (B, 16, 3, 224, 224)\n",
        "print(\"Batch y shape:\", yb.shape)\n",
        "print(\"Sample labels:\", [idx_to_class[int(i)] for i in yb])\n"
      ],
      "metadata": {
        "id": "ZLaQkXWNrNDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ba5858-7d8a-416e-c08f-20dd3c3d365e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class mapping: {'emotion': 0, 'social': 1, 'physical': 2, 'pose_idle': 3}\n",
            "Batch x shape: torch.Size([4, 16, 3, 224, 224])\n",
            "Batch y shape: torch.Size([4])\n",
            "Sample labels: ['social', 'pose_idle', 'physical', 'social']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/synthetic_videos\n"
      ],
      "metadata": {
        "id": "NljTPG7ty5RA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f36fc8-ad22-4154-9eb4-7bfe7b8b2760"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " air_squat0001-0112.mp4\n",
            " Angry0001-0899.mp4\n",
            " Angry0001-1151.mp4\n",
            " Boxing0001-0126.mp4\n",
            " Ch08_nonPBR0001-0002.mp4\n",
            " Cheering0001-0088.mp4\n",
            " Clapping0001-0070.mp4\n",
            " Crying0001-0189.mp4\n",
            " Crying0001-0376.mp4\n",
            " dancing_twerk0001-0912.mp4\n",
            " edge_slip0001-0148.mp4\n",
            " Excited0001-0198.mp4\n",
            " falling_from_losing_balance0001-0401.mp4\n",
            " frames\n",
            " hip_hop_dancing0001-0202.mp4\n",
            " Jogging0001-0078.mp4\n",
            " Jogging0001-0154.mp4\n",
            " Jumping0001-0114.mp4\n",
            " Jumping0001-0158.mp4\n",
            " labels_clean.csv\n",
            " labels.csv\n",
            " Laughing0001-0589.mp4\n",
            " Leaning0001-0201.mp4\n",
            " leaning_on_a_wall0001-0097.mp4\n",
            " leaning_on_a_wall0001-0116.mp4\n",
            " look_around0001-0801.mp4\n",
            "'male_batch2__Mma Kick0001-0267.mp4'\n",
            " male_batch2__Pointing0001-0267.mp4\n",
            " male_batch2__Punching0001-0267.mp4\n",
            " male_batch2__Push0001-0267.mp4\n",
            " male_batch2__Reacting0001-0267.mp4\n",
            " mma_kick0001-0097.mp4\n",
            "'Mma Kick0001-0097.mp4'\n",
            " mma_kick0001-0101.mp4\n",
            "'Mma Kick0001-0267.mp4'\n",
            " nervously_look_around0001-0377.mp4\n",
            " northern_soul_spin0001-0243.mp4\n",
            " Pointing0001-0097.mp4\n",
            " Pointing0001-0216.mp4\n",
            " Pointing0001-0267.mp4\n",
            "'Pointing (1)0001-0207.mp4'\n",
            " pull_heavy_object0001-0097.mp4\n",
            " pulling_a_rope0001-0884.mp4\n",
            " Punching0001-0063.mp4\n",
            " Punching0001-0097.mp4\n",
            " Push0001-0097.mp4\n",
            " Push0001-0481.mp4\n",
            " quick_formal_bow0001-0165.mp4\n",
            " Reacting0001-0097.mp4\n",
            " Reacting0001-0186.mp4\n",
            " Remy0001-0002.mp4\n",
            " Running0001-0043.mp4\n",
            " Running0001-0097.mp4\n",
            " sad_idle0001-0097.mp4\n",
            " sad_idle0001-0161.mp4\n",
            " shaking_hands_20001-0263.mp4\n",
            " shaking_hands_20001-0267.mp4\n",
            " Sitting0001-0267.mp4\n",
            " Sitting0001-0575.mp4\n",
            "'Sitting (1)0001-0267.mp4'\n",
            " sitting_talking0001-2699.mp4\n",
            " split.csv\n",
            " split_grouped.csv\n",
            " split_grouped_final.csv\n",
            " standing_thumbs_up0001-0251.mp4\n",
            " standing_thumbs_up0001-0267.mp4\n",
            " standing_w_briefcase_idle0001-0419.mp4\n",
            " stand_up0001-0267.mp4\n",
            " stand_up0001-0291.mp4\n",
            " start_walking0001-0267.mp4\n",
            " Surprised0001-0251.mp4\n",
            " Talking0001-0179.mp4\n",
            " Talking0001-0267.mp4\n",
            " talking_on_phone0001-2331.mp4\n",
            " T-Pose0001-0002.mp4\n",
            " tut_hip_hop_dance0001-0267.mp4\n",
            " Walking0001-0062.mp4\n",
            " walking_backwards0001-0059.mp4\n",
            " Waving0001-0037.mp4\n",
            "'Waving (1)0001-0191.mp4'\n",
            "'waving(1_hand)0001-0267.mp4'\n",
            "'waving_(2_hands)0001-0267.mp4'\n",
            " Yelling0001-0236.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "FRAMES_ROOT = DATA_ROOT / \"frames\"\n",
        "CSV_PATH = DATA_ROOT / \"split_grouped_final.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "CLASS_NAMES = [\"emotion\", \"social\", \"physical\", \"pose_idle\"]\n",
        "class_to_idx = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
        "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
        "\n",
        "print(\"Class mapping:\", class_to_idx)\n",
        "\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class VideoFramesDataset(Dataset):\n",
        "    def __init__(self, df, split: str):\n",
        "        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        video_path = Path(row[\"video_path\"])\n",
        "        label = row[\"group_label\"]\n",
        "        y = class_to_idx[label]\n",
        "\n",
        "        frames_dir = FRAMES_ROOT / self.split / label / video_path.stem\n",
        "        frame_files = sorted(frames_dir.glob(\"frame_*.jpg\"))\n",
        "\n",
        "        if len(frame_files) != 16:\n",
        "            raise RuntimeError(f\"Expected 16 frames, got {len(frame_files)} for {frames_dir}\")\n",
        "\n",
        "        frames = []\n",
        "        for ff in frame_files:\n",
        "            img = Image.open(ff).convert(\"RGB\")\n",
        "            frames.append(img_tf(img))\n",
        "\n",
        "        x = torch.stack(frames, dim=0)  # (T, C, H, W)\n",
        "        return x, y\n",
        "\n",
        "train_ds = VideoFramesDataset(df, \"train\")\n",
        "val_ds   = VideoFramesDataset(df, \"val\")\n",
        "test_ds  = VideoFramesDataset(df, \"test\")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"Batch x shape:\", xb.shape)\n",
        "print(\"Batch y shape:\", yb.shape)\n",
        "print(\"Sample labels:\", [idx_to_class[int(i)] for i in yb])\n"
      ],
      "metadata": {
        "id": "PyP2Uim-1oyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0768e981-4a82-41a6-8fa3-7267d4519a88"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class mapping: {'emotion': 0, 'social': 1, 'physical': 2, 'pose_idle': 3}\n",
            "Batch x shape: torch.Size([4, 16, 3, 224, 224])\n",
            "Batch y shape: torch.Size([4])\n",
            "Sample labels: ['emotion', 'pose_idle', 'pose_idle', 'social']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw7XKWBR7oK0",
        "outputId": "b521d05f-e43c-403e-a53d-da91a46903a9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetLSTM(nn.Module):\n",
        "    def __init__(self, num_classes=4, hidden_size=256, num_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pretrained ResNet18 backbone\n",
        "        base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        self.feature_extractor = nn.Sequential(*list(base.children())[:-1])  # remove FC\n",
        "\n",
        "        feat_dim = 512  # resnet18 final feature size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=feat_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B*T, C, H, W)\n",
        "\n",
        "        with torch.no_grad():  # start stable: freeze resnet initially\n",
        "            feats = self.feature_extractor(x).view(B, T, -1)  # (B, T, 512)\n",
        "\n",
        "        out, _ = self.lstm(feats)  # (B, T, hidden)\n",
        "        last = out[:, -1, :]       # (B, hidden)\n",
        "        logits = self.classifier(last)\n",
        "        return logits\n",
        "\n",
        "model = ResNetLSTM(num_classes=4).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Rs-_bI79UW",
        "outputId": "b4909f06-9279-423c-e7f0-e920ded01e72"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNetLSTM(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (lstm): LSTM(512, 256, batch_first=True)\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.2, inplace=False)\n",
            "    (1): Linear(in_features=256, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, y):\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ---- Train ----\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    n_train = 0\n",
        "\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch} Train\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        train_loss += loss.item() * bs\n",
        "        train_acc += accuracy(logits, yb) * bs\n",
        "        n_train += bs\n",
        "\n",
        "    train_loss /= n_train\n",
        "    train_acc /= n_train\n",
        "\n",
        "    # ---- Val ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    n_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch} Val\"):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            val_loss += loss.item() * bs\n",
        "            val_acc += accuracy(logits, yb) * bs\n",
        "            n_val += bs\n",
        "\n",
        "    val_loss /= n_val\n",
        "    val_acc /= n_val\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}: Train loss={train_loss:.4f}, acc={train_acc:.3f} | Val loss={val_loss:.4f}, acc={val_acc:.3f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fgMki188BI7",
        "outputId": "f4dc9e12-c9f6-483c-c58c-560a6f2c31c0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Train: 100%|██████████| 14/14 [00:04<00:00,  2.85it/s]\n",
            "Epoch 1 Val: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Train loss=1.4279, acc=0.340 | Val loss=1.3165, acc=0.417\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Train: 100%|██████████| 14/14 [00:03<00:00,  3.55it/s]\n",
            "Epoch 2 Val: 100%|██████████| 3/3 [00:00<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Train loss=1.2983, acc=0.434 | Val loss=1.3454, acc=0.333\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Train: 100%|██████████| 14/14 [00:03<00:00,  4.03it/s]\n",
            "Epoch 3 Val: 100%|██████████| 3/3 [00:00<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Train loss=1.2469, acc=0.509 | Val loss=1.3351, acc=0.583\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Train: 100%|██████████| 14/14 [00:04<00:00,  3.23it/s]\n",
            "Epoch 4 Val: 100%|██████████| 3/3 [00:01<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Train loss=1.2242, acc=0.453 | Val loss=1.3236, acc=0.417\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Train: 100%|██████████| 14/14 [00:03<00:00,  3.79it/s]\n",
            "Epoch 5 Val: 100%|██████████| 3/3 [00:00<00:00,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: Train loss=1.1538, acc=0.547 | Val loss=1.3087, acc=0.500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(yb.cpu().numpy())\n",
        "\n",
        "acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "print(\"Test accuracy:\", acc)\n",
        "\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "print(\"\\nConfusion matrix:\\n\", cm)\n",
        "\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(all_targets, all_preds, target_names=CLASS_NAMES))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KT7xxja8Fut",
        "outputId": "53b1eb70-5ff7-4978-980b-c6734e89c787"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.3333333333333333\n",
            "\n",
            "Confusion matrix:\n",
            " [[0 0 0 1]\n",
            " [0 0 0 4]\n",
            " [0 0 1 3]\n",
            " [0 0 0 3]]\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     emotion       0.00      0.00      0.00         1\n",
            "      social       0.00      0.00      0.00         4\n",
            "    physical       1.00      0.25      0.40         4\n",
            "   pose_idle       0.27      1.00      0.43         3\n",
            "\n",
            "    accuracy                           0.33        12\n",
            "   macro avg       0.32      0.31      0.21        12\n",
            "weighted avg       0.40      0.33      0.24        12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.feature_extractor.named_parameters():\n",
        "    if \"7\" in name:   # layer4 is index 7 in our Sequential\n",
        "        param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "print(\"Unfroze ResNet layer4 and reduced LR.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhDAYEZK8n-p",
        "outputId": "5f122a81-fb44-42fa-a034-f24190713b42"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfroze ResNet layer4 and reduced LR.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss = train_acc = n_train = 0\n",
        "\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"FT Epoch {epoch} Train\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        train_loss += loss.item() * bs\n",
        "        train_acc += accuracy(logits, yb) * bs\n",
        "        n_train += bs\n",
        "\n",
        "    train_loss /= n_train\n",
        "    train_acc /= n_train\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = val_acc = n_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            val_loss += loss.item() * bs\n",
        "            val_acc += accuracy(logits, yb) * bs\n",
        "            n_val += bs\n",
        "\n",
        "    val_loss /= n_val\n",
        "    val_acc /= n_val\n",
        "\n",
        "    print(f\"\\nFT Epoch {epoch}: Train acc={train_acc:.3f} | Val acc={val_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14BEZDbN8sNw",
        "outputId": "7b5ccb93-2c31-45f1-ff42-38ec347357b3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 1 Train: 100%|██████████| 14/14 [00:03<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 1: Train acc=0.566 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 2 Train: 100%|██████████| 14/14 [00:04<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 2: Train acc=0.660 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 3 Train: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 3: Train acc=0.623 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 4 Train: 100%|██████████| 14/14 [00:03<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 4: Train acc=0.717 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 5 Train: 100%|██████████| 14/14 [00:04<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 5: Train acc=0.604 | Val acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 6 Train: 100%|██████████| 14/14 [00:03<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 6: Train acc=0.679 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 7 Train: 100%|██████████| 14/14 [00:03<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 7: Train acc=0.642 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 8 Train: 100%|██████████| 14/14 [00:03<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 8: Train acc=0.736 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 9 Train: 100%|██████████| 14/14 [00:03<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 9: Train acc=0.717 | Val acc=0.583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 10 Train: 100%|██████████| 14/14 [00:03<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 10: Train acc=0.679 | Val acc=0.417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 11 Train: 100%|██████████| 14/14 [00:03<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 11: Train acc=0.811 | Val acc=0.417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 12 Train: 100%|██████████| 14/14 [00:03<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 12: Train acc=0.642 | Val acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 13 Train: 100%|██████████| 14/14 [00:03<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 13: Train acc=0.755 | Val acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 14 Train: 100%|██████████| 14/14 [00:03<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 14: Train acc=0.774 | Val acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FT Epoch 15 Train: 100%|██████████| 14/14 [00:03<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FT Epoch 15: Train acc=0.811 | Val acc=0.500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(yb.cpu().numpy())\n",
        "\n",
        "acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "print(\"Test accuracy:\", acc)\n",
        "\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "print(\"\\nConfusion matrix:\\n\", cm)\n",
        "\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(all_targets, all_preds, target_names=CLASS_NAMES))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FIRkJyA8yE_",
        "outputId": "31561221-ece6-4119-954a-a01a27431f61"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.4166666666666667\n",
            "\n",
            "Confusion matrix:\n",
            " [[1 0 0 0]\n",
            " [0 0 1 3]\n",
            " [0 0 2 2]\n",
            " [0 0 1 2]]\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     emotion       1.00      1.00      1.00         1\n",
            "      social       0.00      0.00      0.00         4\n",
            "    physical       0.50      0.50      0.50         4\n",
            "   pose_idle       0.29      0.67      0.40         3\n",
            "\n",
            "    accuracy                           0.42        12\n",
            "   macro avg       0.45      0.54      0.47        12\n",
            "weighted avg       0.32      0.42      0.35        12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/synthetic_videos/resnet_lstm_grouped_ft.pt\"\n",
        "torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"class_names\": CLASS_NAMES,\n",
        "    \"class_to_idx\": class_to_idx\n",
        "}, save_path)\n",
        "print(\"Saved:\", save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzFDKOvj-TJs",
        "outputId": "c7b05354-ff2c-493b-d06d-174e008f9a3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/synthetic_videos/resnet_lstm_grouped_ft.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "train_df = df[df[\"split\"] == \"train\"]\n",
        "counts = train_df[\"group_label\"].value_counts()\n",
        "\n",
        "weights = []\n",
        "for c in CLASS_NAMES:\n",
        "    weights.append(1.0 / counts.get(c, 1))\n",
        "\n",
        "weights = np.array(weights, dtype=np.float32)\n",
        "weights = weights / weights.sum() * len(CLASS_NAMES)   # normalize\n",
        "\n",
        "weights_t = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Train class counts:\\n\", counts)\n",
        "print(\"Class weights:\", weights_t)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_t)\n",
        "print(\"Updated criterion to weighted CrossEntropyLoss.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCGkJGEN-t5b",
        "outputId": "0cb69cdb-4bdc-4c8a-85f0-070fff003af5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class counts:\n",
            " group_label\n",
            "pose_idle    21\n",
            "physical     14\n",
            "social       11\n",
            "emotion       7\n",
            "Name: count, dtype: int64\n",
            "Class weights: tensor([1.6196, 1.0307, 0.8098, 0.5399], device='cuda:0')\n",
            "Updated criterion to weighted CrossEntropyLoss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "def accuracy(logits, y):\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss = train_acc = n_train = 0\n",
        "\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"WFT Epoch {epoch} Train\"):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        train_loss += loss.item() * bs\n",
        "        train_acc += accuracy(logits, yb) * bs\n",
        "        n_train += bs\n",
        "\n",
        "    train_loss /= n_train\n",
        "    train_acc /= n_train\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = val_acc = n_val = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            val_loss += loss.item() * bs\n",
        "            val_acc += accuracy(logits, yb) * bs\n",
        "            n_val += bs\n",
        "\n",
        "    val_loss /= n_val\n",
        "    val_acc /= n_val\n",
        "\n",
        "    print(f\"WFT Epoch {epoch}: Train acc={train_acc:.3f} | Val acc={val_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVW8XW7h_Y3N",
        "outputId": "637499dd-0df9-4caa-96ec-ffadea2f7d6d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WFT Epoch 1 Train: 100%|██████████| 14/14 [00:04<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WFT Epoch 1: Train acc=0.792 | Val acc=0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WFT Epoch 2 Train: 100%|██████████| 14/14 [00:03<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WFT Epoch 2: Train acc=0.811 | Val acc=0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WFT Epoch 3 Train: 100%|██████████| 14/14 [00:03<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WFT Epoch 3: Train acc=0.830 | Val acc=0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WFT Epoch 4 Train: 100%|██████████| 14/14 [00:04<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WFT Epoch 4: Train acc=0.811 | Val acc=0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WFT Epoch 5 Train: 100%|██████████| 14/14 [00:03<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WFT Epoch 5: Train acc=0.887 | Val acc=0.250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics\n",
        "from ultralytics import YOLO\n",
        "yolo = YOLO(\"yolov8n.pt\")  # downloads weights first time\n",
        "print(\"YOLO ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yflQE_B__cp8",
        "outputId": "e5cbb014-0223-4bde-beb7-9baa70b3ea55"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 217.2MB/s 0.0s\n",
            "YOLO ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive/synthetic_videos/frames -type f | wc -l\n",
        "!ls /content/drive/MyDrive/synthetic_videos/frames\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb9j2VZRIcII",
        "outputId": "d460ff96-87f5-4f2a-96e2-1531ef098097"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1232\n",
            "test  train  val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "FRAMES_ROOT = DATA_ROOT / \"frames\"\n",
        "PERSON_ROOT = DATA_ROOT / \"frames_person\"\n",
        "\n",
        "yolo = YOLO(\"yolov8n.pt\")\n",
        "print(\"YOLO loaded\")\n",
        "\n",
        "def crop_person_with_yolo(pil_img, conf=0.25):\n",
        "    img = np.array(pil_img)\n",
        "    results = yolo.predict(img, conf=conf, verbose=False)\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    best = None\n",
        "    best_area = -1\n",
        "\n",
        "    for r in results:\n",
        "        if r.boxes is None:\n",
        "            continue\n",
        "        for b in r.boxes:\n",
        "            if int(b.cls.item()) == 0:  # person\n",
        "                x1, y1, x2, y2 = b.xyxy[0].cpu().numpy().astype(int)\n",
        "                x1, y1 = max(0, x1), max(0, y1)\n",
        "                x2, y2 = min(w, x2), min(h, y2)\n",
        "                area = (x2 - x1) * (y2 - y1)\n",
        "                if area > best_area:\n",
        "                    best_area = area\n",
        "                    best = (x1, y1, x2, y2)\n",
        "\n",
        "    if best is None:\n",
        "        return pil_img  # fallback\n",
        "\n",
        "    return pil_img.crop(best)\n",
        "\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "total_done = 0\n",
        "\n",
        "for split in splits:\n",
        "    split_dir = FRAMES_ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        raise FileNotFoundError(f\"Missing: {split_dir}\")\n",
        "\n",
        "    for group_dir in split_dir.iterdir():\n",
        "        if not group_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        for video_dir in group_dir.iterdir():\n",
        "            if not video_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            out_dir = PERSON_ROOT / split / group_dir.name / video_dir.name\n",
        "            out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            frame_files = sorted(video_dir.glob(\"frame_*.jpg\"))\n",
        "            for ff in frame_files:\n",
        "                out_path = out_dir / ff.name\n",
        "                if out_path.exists():\n",
        "                    continue\n",
        "\n",
        "                img = Image.open(ff).convert(\"RGB\")\n",
        "                crop = crop_person_with_yolo(img, conf=0.25)\n",
        "                crop = crop.resize((224, 224))\n",
        "                crop.save(out_path, quality=95)\n",
        "                total_done += 1\n",
        "\n",
        "print(\"Done. Newly saved crops:\", total_done)\n",
        "print(\"Person crops folder:\", PERSON_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twzxzQ73I4a0",
        "outputId": "dab6b248-ed41-4043-c9ee-61c114e9aee7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO loaded\n",
            "Done. Newly saved crops: 1232\n",
            "Person crops folder: /content/drive/MyDrive/synthetic_videos/frames_person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive/synthetic_videos/frames_person -type f | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ptmL-HTI9Ze",
        "outputId": "945fd3ff-8a77-475f-de17-b1dac859dd4b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "FRAMES_ROOT = DATA_ROOT / \"frames\"\n",
        "PERSON_ROOT = DATA_ROOT / \"frames_person\"\n",
        "\n",
        "IMG_SIZE = 224\n",
        "NUM_FRAMES = 16\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class TwoStreamFramesDataset(Dataset):\n",
        "    def __init__(self, df, split, class_to_idx, num_frames=16):\n",
        "        self.df = df[df[\"split\"] == split].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "    def _load_clip(self, base_root, group_label, video_stem):\n",
        "        clip_dir = base_root / self.split / group_label / video_stem\n",
        "        frames = sorted(clip_dir.glob(\"frame_*.jpg\"))\n",
        "        if len(frames) < self.num_frames:\n",
        "            raise FileNotFoundError(f\"Not enough frames in {clip_dir} (found {len(frames)})\")\n",
        "\n",
        "        # take first num_frames (consistent)\n",
        "        frames = frames[:self.num_frames]\n",
        "\n",
        "        imgs = []\n",
        "        for f in frames:\n",
        "            img = Image.open(f).convert(\"RGB\")\n",
        "            img = transform(img)\n",
        "            imgs.append(img)\n",
        "        # [T, C, H, W]\n",
        "        return torch.stack(imgs, dim=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        group_label = row[\"group_label\"]\n",
        "        video_path = Path(row[\"video_path\"])\n",
        "        video_stem = video_path.stem\n",
        "\n",
        "        x_global = self._load_clip(FRAMES_ROOT, group_label, video_stem)\n",
        "        x_person = self._load_clip(PERSON_ROOT, group_label, video_stem)\n",
        "\n",
        "        y = self.class_to_idx[group_label]\n",
        "        return x_global, x_person, torch.tensor(y, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "qPhJKVbWJe0i"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class TwoStreamResNetLSTM(nn.Module):\n",
        "    def __init__(self, num_classes, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        backbone.fc = nn.Identity()  # output 512\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.lstm = nn.LSTM(input_size=1024, hidden_size=hidden, batch_first=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, num_classes)\n",
        "        )\n",
        "\n",
        "    def extract_feat_seq(self, x):\n",
        "        # x: [B, T, C, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B*T, C, H, W)\n",
        "        feats = self.backbone(x)      # [B*T, 512]\n",
        "        feats = feats.view(B, T, 512) # [B, T, 512]\n",
        "        return feats\n",
        "\n",
        "    def forward(self, x_global, x_person):\n",
        "        f1 = self.extract_feat_seq(x_global)\n",
        "        f2 = self.extract_feat_seq(x_person)\n",
        "        f = torch.cat([f1, f2], dim=-1)   # [B, T, 1024]\n",
        "        out, _ = self.lstm(f)\n",
        "        last = out[:, -1, :]              # [B, hidden]\n",
        "        return self.classifier(last)\n"
      ],
      "metadata": {
        "id": "YQbeBSOcKOYP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = TwoStreamFramesDataset(df, \"train\", class_to_idx, num_frames=NUM_FRAMES)\n",
        "val_ds   = TwoStreamFramesDataset(df, \"val\",   class_to_idx, num_frames=NUM_FRAMES)\n",
        "test_ds  = TwoStreamFramesDataset(df, \"test\",  class_to_idx, num_frames=NUM_FRAMES)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "xb1, xb2, yb = next(iter(train_loader))\n",
        "print(\"Global:\", xb1.shape, \"Person:\", xb2.shape, \"y:\", yb.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17CsNY3TKQox",
        "outputId": "5104e49d-2a00-49c5-d8f6-1410d400aa3e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global: torch.Size([4, 16, 3, 224, 224]) Person: torch.Size([4, 16, 3, 224, 224]) y: torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model2 = TwoStreamResNetLSTM(num_classes=len(CLASS_NAMES)).to(device)\n",
        "\n",
        "# weighted loss (keep it)\n",
        "train_df = df[df[\"split\"] == \"train\"]\n",
        "counts = train_df[\"group_label\"].value_counts()\n",
        "weights = np.array([1.0 / counts.get(c, 1) for c in CLASS_NAMES], dtype=np.float32)\n",
        "weights = weights / weights.sum() * len(CLASS_NAMES)\n",
        "weights_t = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_t)\n",
        "optimizer = optim.Adam(model2.parameters(), lr=5e-5)\n",
        "\n",
        "def acc_from_logits(logits, y):\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "EPOCHS = 10\n",
        "best_val = -1\n",
        "best_path = \"/content/drive/MyDrive/synthetic_videos/twostream_best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model2.train()\n",
        "    tr_loss = tr_acc = n = 0\n",
        "\n",
        "    for xg, xp, y in tqdm(train_loader, desc=f\"Epoch {epoch} Train\"):\n",
        "        xg, xp, y = xg.to(device), xp.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model2(xg, xp)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        tr_loss += loss.item() * bs\n",
        "        tr_acc += acc_from_logits(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    tr_loss /= n\n",
        "    tr_acc /= n\n",
        "\n",
        "    model2.eval()\n",
        "    va_loss = va_acc = n2 = 0\n",
        "    with torch.no_grad():\n",
        "        for xg, xp, y in val_loader:\n",
        "            xg, xp, y = xg.to(device), xp.to(device), y.to(device)\n",
        "            logits = model2(xg, xp)\n",
        "            loss = criterion(logits, y)\n",
        "            bs = y.size(0)\n",
        "            va_loss += loss.item() * bs\n",
        "            va_acc += acc_from_logits(logits, y) * bs\n",
        "            n2 += bs\n",
        "\n",
        "    va_loss /= n2\n",
        "    va_acc /= n2\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train acc={tr_acc:.3f} | Val acc={va_acc:.3f}\")\n",
        "\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        torch.save({\n",
        "            \"model_state\": model2.state_dict(),\n",
        "            \"class_names\": CLASS_NAMES,\n",
        "            \"class_to_idx\": class_to_idx\n",
        "        }, best_path)\n",
        "        print(\"Saved best to:\", best_path)\n",
        "\n",
        "print(\"Best val:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpJjdAWKT6_",
        "outputId": "e37f66b8-152a-442a-d697-5df1f375ff78"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Train: 100%|██████████| 14/14 [00:21<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train acc=0.396 | Val acc=0.333\n",
            "Saved best to: /content/drive/MyDrive/synthetic_videos/twostream_best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Train: 100%|██████████| 14/14 [00:12<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train acc=0.717 | Val acc=0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Train: 100%|██████████| 14/14 [00:12<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train acc=0.906 | Val acc=0.417\n",
            "Saved best to: /content/drive/MyDrive/synthetic_videos/twostream_best.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Train: 100%|██████████| 14/14 [00:12<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train acc=0.868 | Val acc=0.417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Train: 100%|██████████| 14/14 [00:12<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train acc=1.000 | Val acc=0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Train: 100%|██████████| 14/14 [00:12<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train acc=1.000 | Val acc=0.333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Train: 100%|██████████| 14/14 [00:12<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train acc=0.962 | Val acc=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Train: 100%|██████████| 14/14 [00:13<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train acc=1.000 | Val acc=0.417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Train: 100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train acc=1.000 | Val acc=0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Train: 100%|██████████| 14/14 [00:12<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train acc=1.000 | Val acc=0.167\n",
            "Best val: 0.4166666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load best\n",
        "ckpt = torch.load(best_path, map_location=device)\n",
        "model2.load_state_dict(ckpt[\"model_state\"])\n",
        "model2.eval()\n",
        "\n",
        "all_preds, all_targets = [], []\n",
        "with torch.no_grad():\n",
        "    for xg, xp, y in test_loader:\n",
        "        xg, xp, y = xg.to(device), xp.to(device), y.to(device)\n",
        "        logits = model2(xg, xp)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "print(\"Two-stream Test accuracy:\", acc)\n",
        "\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "print(\"\\nConfusion matrix:\\n\", cm)\n",
        "\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(all_targets, all_preds, target_names=CLASS_NAMES))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGkMKvamKZ-e",
        "outputId": "a4e9333c-6a9d-4d27-f61c-655718690eac"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two-stream Test accuracy: 0.3333333333333333\n",
            "\n",
            "Confusion matrix:\n",
            " [[1 0 0 0]\n",
            " [1 0 1 2]\n",
            " [0 0 2 2]\n",
            " [2 0 0 1]]\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     emotion       0.25      1.00      0.40         1\n",
            "      social       0.00      0.00      0.00         4\n",
            "    physical       0.67      0.50      0.57         4\n",
            "   pose_idle       0.20      0.33      0.25         3\n",
            "\n",
            "    accuracy                           0.33        12\n",
            "   macro avg       0.28      0.46      0.31        12\n",
            "weighted avg       0.29      0.33      0.29        12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "PERSON_ROOT = DATA_ROOT / \"frames_person\"\n",
        "POSE_ROOT = DATA_ROOT / \"pose_keypoints\"\n",
        "\n",
        "pose_model = YOLO(\"yolov8n-pose.pt\")   # light + fast\n",
        "print(\"Pose model loaded \")\n",
        "print(\"PERSON_ROOT:\", PERSON_ROOT)\n",
        "print(\"POSE_ROOT:\", POSE_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6j5D4HUMF1o",
        "outputId": "66e439a5-cdba-4aac-ad74-419876485cb6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n-pose.pt to 'yolov8n-pose.pt': 100% ━━━━━━━━━━━━ 6.5MB 314.3MB/s 0.0s\n",
            "Pose model loaded \n",
            "PERSON_ROOT: /content/drive/MyDrive/synthetic_videos/frames_person\n",
            "POSE_ROOT: /content/drive/MyDrive/synthetic_videos/pose_keypoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pose_from_image(img_path, max_people=2, conf=0.25):\n",
        "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    res = pose_model.predict(img, conf=conf, verbose=False)[0]\n",
        "\n",
        "    # If no detections\n",
        "    if res.keypoints is None or len(res.keypoints) == 0:\n",
        "        # persons, kpts, (x,y,conf)\n",
        "        return np.zeros((0, 17, 3), dtype=np.float32)\n",
        "\n",
        "    kpts = res.keypoints.data.cpu().numpy()  # [N,17,3]\n",
        "    # Sort persons by bbox area (largest first) so consistent selection\n",
        "    if res.boxes is not None and len(res.boxes) == len(kpts):\n",
        "        xyxy = res.boxes.xyxy.cpu().numpy()\n",
        "        areas = (xyxy[:,2]-xyxy[:,0])*(xyxy[:,3]-xyxy[:,1])\n",
        "        order = np.argsort(-areas)\n",
        "        kpts = kpts[order]\n",
        "\n",
        "    return kpts[:max_people].astype(np.float32)\n",
        "\n",
        "processed = 0\n",
        "skipped = 0\n",
        "\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "for split in splits:\n",
        "    split_dir = PERSON_ROOT / split\n",
        "    if not split_dir.exists():\n",
        "        raise FileNotFoundError(f\"Missing: {split_dir}\")\n",
        "\n",
        "    for group_dir in split_dir.iterdir():\n",
        "        if not group_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        for video_dir in group_dir.iterdir():\n",
        "            if not video_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            out_dir = POSE_ROOT / split / group_dir.name / video_dir.name\n",
        "            out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            for frame_path in sorted(video_dir.glob(\"frame_*.jpg\")):\n",
        "                out_path = out_dir / (frame_path.stem + \".npz\")\n",
        "                if out_path.exists():\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "\n",
        "                kpts = extract_pose_from_image(frame_path, max_people=2, conf=0.25)\n",
        "                np.savez_compressed(out_path, kpts=kpts)\n",
        "                processed += 1\n",
        "\n",
        "print(\"Pose extraction done \")\n",
        "print(\"Newly processed frames:\", processed)\n",
        "print(\"Skipped existing:\", skipped)\n",
        "print(\"Saved at:\", POSE_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrDKp3UFMHo-",
        "outputId": "998e73d4-9029-4695-9f4e-816ad70b2ced"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pose extraction done \n",
            "Newly processed frames: 1232\n",
            "Skipped existing: 0\n",
            "Saved at: /content/drive/MyDrive/synthetic_videos/pose_keypoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count pose files\n",
        "!find /content/drive/MyDrive/synthetic_videos/pose_keypoints -type f | wc -l\n",
        "\n",
        "# Inspect one file shape\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "any_file = next(Path(\"/content/drive/MyDrive/synthetic_videos/pose_keypoints\").rglob(\"*.npz\"))\n",
        "data = np.load(any_file)\n",
        "print(\"Example:\", any_file)\n",
        "print(\"kpts shape:\", data[\"kpts\"].shape)   # (0..2, 17, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwSCS_snML0H",
        "outputId": "62ce1d2a-9c8e-44cc-9aeb-5c578e419743"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1232\n",
            "Example: /content/drive/MyDrive/synthetic_videos/pose_keypoints/train/emotion/Angry0001-1151/frame_00.npz\n",
            "kpts shape: (0, 17, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "FRAMES_ROOT = DATA_ROOT / \"frames\"\n",
        "POSE_FULL_ROOT = DATA_ROOT / \"pose_keypoints_full\"\n",
        "\n",
        "pose_model = YOLO(\"yolov8n-pose.pt\")\n",
        "print(\"Pose model loaded ✅\")\n",
        "\n",
        "def extract_pose(img_path, max_people=2, conf=0.25):\n",
        "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    res = pose_model.predict(img, conf=conf, verbose=False)[0]\n",
        "\n",
        "    if res.keypoints is None or len(res.keypoints) == 0:\n",
        "        return np.zeros((0, 17, 3), dtype=np.float32)\n",
        "\n",
        "    kpts = res.keypoints.data.cpu().numpy()  # [N,17,3]\n",
        "\n",
        "    # sort by bbox area, keep top2\n",
        "    if res.boxes is not None and len(res.boxes) == len(kpts):\n",
        "        xyxy = res.boxes.xyxy.cpu().numpy()\n",
        "        areas = (xyxy[:,2]-xyxy[:,0])*(xyxy[:,3]-xyxy[:,1])\n",
        "        order = np.argsort(-areas)\n",
        "        kpts = kpts[order]\n",
        "\n",
        "    return kpts[:max_people].astype(np.float32)\n",
        "\n",
        "processed = 0\n",
        "splits = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "for split in splits:\n",
        "    for group_dir in (FRAMES_ROOT / split).iterdir():\n",
        "        if not group_dir.is_dir():\n",
        "            continue\n",
        "        for video_dir in group_dir.iterdir():\n",
        "            if not video_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            out_dir = POSE_FULL_ROOT / split / group_dir.name / video_dir.name\n",
        "            out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            for frame_path in sorted(video_dir.glob(\"frame_*.jpg\")):\n",
        "                out_path = out_dir / (frame_path.stem + \".npz\")\n",
        "                if out_path.exists():\n",
        "                    continue\n",
        "\n",
        "                kpts = extract_pose(frame_path, max_people=2, conf=0.25)\n",
        "                np.savez_compressed(out_path, kpts=kpts)\n",
        "                processed += 1\n",
        "\n",
        "print(\"Done  newly processed:\", processed)\n",
        "print(\"Saved at:\", POSE_FULL_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k62lP2ORMXpN",
        "outputId": "3e2f0441-21ca-4325-de78-74252ad0762f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pose model loaded ✅\n",
            "Done  newly processed: 1232\n",
            "Saved at: /content/drive/MyDrive/synthetic_videos/pose_keypoints_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "ROOT = Path(\"/content/drive/MyDrive/synthetic_videos/pose_keypoints_full\")\n",
        "files = list(ROOT.rglob(\"*.npz\"))\n",
        "print(\"Total pose files:\", len(files))\n",
        "\n",
        "sample = random.sample(files, 20)\n",
        "zeros = 0\n",
        "for f in sample:\n",
        "    k = np.load(f)[\"kpts\"]\n",
        "    if k.shape[0] == 0:\n",
        "        zeros += 1\n",
        "\n",
        "print(\"Zero-detection in sample:\", zeros, \"/ 20\")\n",
        "print(\"Example file:\", sample[0], \"shape:\", np.load(sample[0])[\"kpts\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuiocKb8Q4qi",
        "outputId": "95cf9d4f-09e1-4135-fe98-8d37f2058640"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pose files: 1232\n",
            "Zero-detection in sample: 6 / 20\n",
            "Example file: /content/drive/MyDrive/synthetic_videos/pose_keypoints_full/test/pose_idle/leaning_on_a_wall0001-0116/frame_01.npz shape: (1, 17, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "CSV_PATH  = DATA_ROOT / \"split_grouped_final.csv\"\n",
        "POSE_ROOT = DATA_ROOT / \"pose_keypoints_full\"   # IMPORTANT: full poses\n",
        "OUT_FEATS = DATA_ROOT / \"pose_features_16f.npz\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Ensure consistent\n",
        "df[\"video_stem\"] = df[\"video_path\"].apply(lambda p: Path(p).stem)\n",
        "\n",
        "CLASS_NAMES = [\"emotion\", \"social\", \"physical\", \"pose_idle\"]\n",
        "class_to_id = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
        "\n",
        "# COCO keypoint indices (YOLOv8-pose uses COCO order)\n",
        "# 0 nose, 5 left_shoulder, 6 right_shoulder, 9 left_wrist, 10 right_wrist, 11 left_hip, 12 right_hip\n",
        "L_SH, R_SH = 5, 6\n",
        "L_WR, R_WR = 9, 10\n",
        "\n",
        "def kpts_to_bbox_xyxy(kpts_xy):\n",
        "    # kpts_xy: (17,2)\n",
        "    xs = kpts_xy[:,0]; ys = kpts_xy[:,1]\n",
        "    # if all zero -> empty bbox\n",
        "    if np.all(xs == 0) and np.all(ys == 0):\n",
        "        return np.array([0,0,0,0], dtype=np.float32)\n",
        "    x1, y1 = xs.min(), ys.min()\n",
        "    x2, y2 = xs.max(), ys.max()\n",
        "    return np.array([x1,y1,x2,y2], dtype=np.float32)\n",
        "\n",
        "def safe_unit(v, eps=1e-6):\n",
        "    n = np.linalg.norm(v)\n",
        "    if n < eps:\n",
        "        return np.zeros_like(v)\n",
        "    return v / n\n",
        "\n",
        "def frame_features(kpts):\n",
        "    \"\"\"\n",
        "    kpts: (N,17,3) where N in {0,1,2}\n",
        "    output: (73,)\n",
        "    \"\"\"\n",
        "    # Default empty persons\n",
        "    A = np.zeros((17,2), dtype=np.float32)\n",
        "    B = np.zeros((17,2), dtype=np.float32)\n",
        "\n",
        "    if kpts.shape[0] >= 1:\n",
        "        A = kpts[0,:,:2].astype(np.float32)\n",
        "    if kpts.shape[0] >= 2:\n",
        "        B = kpts[1,:,:2].astype(np.float32)\n",
        "\n",
        "    # Relation features\n",
        "    rel = np.zeros((5,), dtype=np.float32)\n",
        "\n",
        "    # centres from bbox\n",
        "    bboxA = kpts_to_bbox_xyxy(A)\n",
        "    bboxB = kpts_to_bbox_xyxy(B)\n",
        "\n",
        "    cA = np.array([(bboxA[0]+bboxA[2])/2, (bboxA[1]+bboxA[3])/2], dtype=np.float32)\n",
        "    cB = np.array([(bboxB[0]+bboxB[2])/2, (bboxB[1]+bboxB[3])/2], dtype=np.float32)\n",
        "\n",
        "    # centre distance (if B exists)\n",
        "    if not np.all(B == 0):\n",
        "        rel[0] = np.linalg.norm(cA - cB)\n",
        "\n",
        "        # wrist distances\n",
        "        rel[1] = np.linalg.norm(A[L_WR] - B[L_WR])\n",
        "        rel[2] = np.linalg.norm(A[R_WR] - B[R_WR])\n",
        "\n",
        "        # facing proxy: compare shoulder direction vectors\n",
        "        vA = safe_unit(A[R_SH] - A[L_SH])\n",
        "        vB = safe_unit(B[R_SH] - B[L_SH])\n",
        "        rel[3] = float(np.dot(vA, vB))  # -1..1\n",
        "\n",
        "        # overlap proxy: bbox IoU-ish (cheap)\n",
        "        xA1,yA1,xA2,yA2 = bboxA\n",
        "        xB1,yB1,xB2,yB2 = bboxB\n",
        "        ix1, iy1 = max(xA1,xB1), max(yA1,yB1)\n",
        "        ix2, iy2 = min(xA2,xB2), min(yA2,yB2)\n",
        "        iw, ih = max(0, ix2-ix1), max(0, iy2-iy1)\n",
        "        inter = iw*ih\n",
        "        areaA = max(0, (xA2-xA1))*max(0, (yA2-yA1))\n",
        "        areaB = max(0, (xB2-xB1))*max(0, (yB2-yB1))\n",
        "        union = areaA + areaB - inter + 1e-6\n",
        "        rel[4] = inter / union\n",
        "\n",
        "    feat = np.concatenate([A.reshape(-1), B.reshape(-1), rel], axis=0)\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "splits = []\n",
        "\n",
        "missing_pose_files = 0\n",
        "zero_pose_frames = 0\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    split = row[\"split\"]\n",
        "    group = row[\"group_label\"]\n",
        "    stem  = row[\"video_stem\"]\n",
        "\n",
        "    # pose folder should match: split/group/video_stem\n",
        "    pose_dir = POSE_ROOT / split / group / stem\n",
        "    if not pose_dir.exists():\n",
        "        missing_pose_files += 1\n",
        "        continue\n",
        "\n",
        "    frames = []\n",
        "    for i in range(16):\n",
        "        f = pose_dir / f\"frame_{i:02d}.npz\"\n",
        "        if not f.exists():\n",
        "            missing_pose_files += 1\n",
        "            kpts = np.zeros((0,17,3), dtype=np.float32)\n",
        "        else:\n",
        "            kpts = np.load(f)[\"kpts\"]\n",
        "        if kpts.shape[0] == 0:\n",
        "            zero_pose_frames += 1\n",
        "        frames.append(frame_features(kpts))\n",
        "\n",
        "    X.append(np.stack(frames, axis=0))  # (16,73)\n",
        "    y.append(class_to_id[row[\"group_label\"]])\n",
        "    splits.append(split)\n",
        "\n",
        "X = np.stack(X, axis=0)  # (N,16,73)\n",
        "y = np.array(y, dtype=np.int64)\n",
        "splits = np.array(splits)\n",
        "\n",
        "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n",
        "print(\"Missing pose files count:\", missing_pose_files)\n",
        "print(\"Total zero-pose frames:\", zero_pose_frames, \"out of\", X.shape[0]*16)\n",
        "\n",
        "np.savez_compressed(OUT_FEATS, X=X, y=y, splits=splits, class_names=np.array(CLASS_NAMES))\n",
        "print(\"Saved features:\", OUT_FEATS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab_hGbYQRAih",
        "outputId": "09fee276-1a73-45c4-d10e-47f04ca8d931"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (77, 16, 73)  y shape: (77,)\n",
            "Missing pose files count: 0\n",
            "Total zero-pose frames: 237 out of 1232\n",
            "Saved features: /content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/drive/MyDrive/synthetic_videos | grep -i \".npz\" || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q859izUlrBWd",
        "outputId": "23a006ce-e8f6-446e-e2fb-03854687be6a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 114K Jan 14 18:00 pose_features_16f.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\"\n",
        "data = np.load(DATA_PATH, allow_pickle=True)\n",
        "\n",
        "print(\"Keys:\", data.files)\n",
        "for k in data.files:\n",
        "    arr = data[k]\n",
        "    print(k, type(arr), getattr(arr, \"shape\", None), getattr(arr, \"dtype\", None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_afJxQetr1t",
        "outputId": "a23d2317-804b-4479-8233-aff7b4b5c6f7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: ['X', 'y', 'splits', 'class_names']\n",
            "X <class 'numpy.ndarray'> (77, 16, 73) float32\n",
            "y <class 'numpy.ndarray'> (77,) int64\n",
            "splits <class 'numpy.ndarray'> (77,) <U5\n",
            "class_names <class 'numpy.ndarray'> (4,) <U9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Load masked pose features\n",
        "# -----------------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\"\n",
        "OUT_BEST  = \"/content/drive/MyDrive/synthetic_videos/pose_transformer_best_weighted.pt\"\n",
        "\n",
        "DATA = np.load(DATA_PATH, allow_pickle=True)\n",
        "X = DATA[\"X\"]          # (N,16,74)\n",
        "y = DATA[\"y\"]          # (N,)\n",
        "spl = DATA[\"splits\"]   # (N,)\n",
        "CLASS_NAMES = list(DATA[\"class_names\"])\n",
        "\n",
        "def idxs(split_name):\n",
        "    return np.where(spl == split_name)[0]\n",
        "\n",
        "train_idx = idxs(\"train\")\n",
        "val_idx   = idxs(\"val\")\n",
        "test_idx  = idxs(\"test\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset / Dataloader\n",
        "# -----------------------------\n",
        "class PoseSeqDS(Dataset):\n",
        "    def __init__(self, X, y, idx):\n",
        "        self.X = torch.tensor(X[idx], dtype=torch.float32)\n",
        "        self.y = torch.tensor(y[idx], dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "train_ds = PoseSeqDS(X, y, train_idx)\n",
        "val_ds   = PoseSeqDS(X, y, val_idx)\n",
        "test_ds  = PoseSeqDS(X, y, test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, d_in=74, d_model=128, nhead=4, num_layers=2, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_in, d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True, dropout=0.2\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(d_model, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)     # (B,T,d_model)\n",
        "        h = self.enc(x)      # (B,T,d_model)\n",
        "        h = h.mean(dim=1)    # (B,d_model)\n",
        "        return self.cls(h)   # (B,num_classes)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = TinyTransformer(d_in=X.shape[-1], num_classes=len(CLASS_NAMES)).to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# Class-weighted loss (IMPORTANT)\n",
        "# -----------------------------\n",
        "train_labels = y[train_idx]\n",
        "counts = np.bincount(train_labels, minlength=len(CLASS_NAMES))  # counts per class\n",
        "weights = 1.0 / (counts + 1e-6)                                 # inverse frequency\n",
        "weights = (weights / weights.sum()) * len(CLASS_NAMES)          # normalized around 1\n",
        "\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "print(\"Train counts:\", counts)\n",
        "print(\"Class weights:\", weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# -----------------------------\n",
        "# Optimizer\n",
        "# -----------------------------\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval loop\n",
        "# -----------------------------\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        loss_sum += loss.item() * yb.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "best_val = 0.0\n",
        "\n",
        "for epoch in range(1, 41):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader, train=False)\n",
        "\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        torch.save(model.state_dict(), OUT_BEST)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | train acc={tr_acc:.3f} val acc={va_acc:.3f} (best {best_val:.3f})\")\n",
        "\n",
        "print(\"Saved best:\", OUT_BEST)\n",
        "print(\"Best val:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6JpY6XFSDWR",
        "outputId": "f659094a-a789-432d-b43c-e426153490c7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train counts: [ 7 11 14 21]\n",
            "Class weights: [     1.6196      1.0307     0.80982     0.53988]\n",
            "Epoch 01 | train acc=0.245 val acc=0.417 (best 0.417)\n",
            "Epoch 05 | train acc=0.283 val acc=0.250 (best 0.417)\n",
            "Epoch 10 | train acc=0.264 val acc=0.500 (best 0.500)\n",
            "Epoch 15 | train acc=0.358 val acc=0.083 (best 0.500)\n",
            "Epoch 20 | train acc=0.358 val acc=0.583 (best 0.583)\n",
            "Epoch 25 | train acc=0.509 val acc=0.250 (best 0.583)\n",
            "Epoch 30 | train acc=0.453 val acc=0.417 (best 0.583)\n",
            "Epoch 35 | train acc=0.396 val acc=0.417 (best 0.583)\n",
            "Epoch 40 | train acc=0.396 val acc=0.083 (best 0.583)\n",
            "Saved best: /content/drive/MyDrive/synthetic_videos/pose_transformer_best_weighted.pt\n",
            "Best val: 0.5833333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/synthetic_videos | grep pose\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL6FsS_hTvRc",
        "outputId": "9bca3dce-6ffa-4d97-bc4d-d9fbaa8a5c62"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pose_features_16f.npz\n",
            "pose_keypoints\n",
            "pose_keypoints_full\n",
            "pose_transformer_best_weighted.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "DATA_PATH = \"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\"\n",
        "OUT_BEST  = \"/content/drive/MyDrive/synthetic_videos/pose_transformer_best_focal_jitter.pt\"\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "DATA = np.load(DATA_PATH, allow_pickle=True)\n",
        "X = DATA[\"X\"]                  # (N, 16, 73)\n",
        "y = DATA[\"y\"]                  # (N,)\n",
        "splits = DATA[\"splits\"]        # (N,) 'train'/'val'/'test'\n",
        "CLASS_NAMES = [str(x) for x in list(DATA[\"class_names\"])]\n",
        "\n",
        "print(\"X:\", X.shape, \"y:\", y.shape, \"classes:\", CLASS_NAMES)\n",
        "\n",
        "def idxs(split_name):\n",
        "    return np.where(splits == split_name)[0]\n",
        "\n",
        "train_idx = idxs(\"train\")\n",
        "val_idx   = idxs(\"val\")\n",
        "test_idx  = idxs(\"test\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset with Temporal Jitter\n",
        "# -----------------------------\n",
        "class PoseSeqDS(Dataset):\n",
        "    def __init__(self, X, y, idx, train_mode=False):\n",
        "        self.X = torch.tensor(X[idx], dtype=torch.float32)\n",
        "        self.y = torch.tensor(y[idx], dtype=torch.long)\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self.X[i].clone()\n",
        "        y = self.y[i]\n",
        "\n",
        "        # Temporal jitter only during training\n",
        "        if self.train_mode:\n",
        "            shift = torch.randint(low=-2, high=3, size=(1,)).item()  # -2..+2\n",
        "            x = torch.roll(x, shifts=shift, dims=0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "train_ds = PoseSeqDS(X, y, train_idx, train_mode=True)\n",
        "val_ds   = PoseSeqDS(X, y, val_idx, train_mode=False)\n",
        "test_ds  = PoseSeqDS(X, y, test_idx, train_mode=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Transformer model\n",
        "# -----------------------------\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, d_in, d_model=128, nhead=4, num_layers=2, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_in, d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, batch_first=True, dropout=0.2\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(d_model, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)      # (B,T,d_model)\n",
        "        h = self.enc(x)       # (B,T,d_model)\n",
        "        h = h.mean(dim=1)     # (B,d_model)\n",
        "        return self.cls(h)    # (B,C)\n",
        "\n",
        "# -----------------------------\n",
        "# Weighted Focal Loss\n",
        "# -----------------------------\n",
        "class WeightedFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # tensor [C] or None\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = nn.functional.cross_entropy(logits, targets, weight=self.alpha, reduction=\"none\")\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        return loss.mean()\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = TinyTransformer(d_in=X.shape[-1], num_classes=len(CLASS_NAMES)).to(device)\n",
        "\n",
        "# class weights from train split\n",
        "train_labels = y[train_idx]\n",
        "counts = np.bincount(train_labels, minlength=len(CLASS_NAMES))\n",
        "weights = 1.0 / (counts + 1e-6)\n",
        "weights = (weights / weights.sum()) * len(CLASS_NAMES)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Train class counts:\", counts)\n",
        "print(\"Class weights:\", weights)\n",
        "\n",
        "criterion = WeightedFocalLoss(alpha=class_weights, gamma=2.0)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "\n",
        "# -----------------------------\n",
        "# Train/Eval loops\n",
        "# -----------------------------\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(train)\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        loss_sum += loss.item() * yb.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "best_val = 0.0\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "for epoch in range(1, 41):\n",
        "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc = run_epoch(val_loader, train=False)\n",
        "\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        torch.save(model.state_dict(), OUT_BEST)\n",
        "\n",
        "    if epoch == 1 or epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:02d} | train acc={tr_acc:.3f} val acc={va_acc:.3f} (best {best_val:.3f})\")\n",
        "\n",
        "print(\"Saved best:\", OUT_BEST)\n",
        "print(\"Best val:\", best_val)\n",
        "\n",
        "model.load_state_dict(torch.load(OUT_BEST, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds = logits.argmax(1).cpu().numpy()\n",
        "        all_preds.extend(list(preds))\n",
        "        all_targets.extend(list(yb.numpy()))\n",
        "\n",
        "acc = (np.array(all_preds) == np.array(all_targets)).mean()\n",
        "print(\"\\nTest accuracy:\", acc)\n",
        "\n",
        "cm = confusion_matrix(all_targets, all_preds, labels=list(range(len(CLASS_NAMES))))\n",
        "print(\"\\nConfusion matrix:\\n\", cm)\n",
        "\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(all_targets, all_preds, target_names=CLASS_NAMES, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bauzu8CkUPGY",
        "outputId": "fca3e695-e283-4bad-f9a6-bc674a9be54f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (77, 16, 73) y: (77,) classes: ['emotion', 'social', 'physical', 'pose_idle']\n",
            "Train class counts: [ 7 11 14 21]\n",
            "Class weights: [     1.6196      1.0307     0.80982     0.53988]\n",
            "Epoch 01 | train acc=0.208 val acc=0.167 (best 0.167)\n",
            "Epoch 05 | train acc=0.170 val acc=0.167 (best 0.250)\n",
            "Epoch 10 | train acc=0.283 val acc=0.250 (best 0.250)\n",
            "Epoch 15 | train acc=0.208 val acc=0.250 (best 0.250)\n",
            "Epoch 20 | train acc=0.302 val acc=0.250 (best 0.250)\n",
            "Epoch 25 | train acc=0.321 val acc=0.250 (best 0.250)\n",
            "Epoch 30 | train acc=0.358 val acc=0.250 (best 0.250)\n",
            "Epoch 35 | train acc=0.377 val acc=0.250 (best 0.250)\n",
            "Epoch 40 | train acc=0.358 val acc=0.250 (best 0.250)\n",
            "Saved best: /content/drive/MyDrive/synthetic_videos/pose_transformer_best_focal_jitter.pt\n",
            "Best val: 0.25\n",
            "\n",
            "Test accuracy: 0.3333333333333333\n",
            "\n",
            "Confusion matrix:\n",
            " [[0 1 0 0]\n",
            " [3 0 0 1]\n",
            " [0 1 3 0]\n",
            " [1 1 0 1]]\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     emotion       0.00      0.00      0.00         1\n",
            "      social       0.00      0.00      0.00         4\n",
            "    physical       1.00      0.75      0.86         4\n",
            "   pose_idle       0.50      0.33      0.40         3\n",
            "\n",
            "    accuracy                           0.33        12\n",
            "   macro avg       0.38      0.27      0.31        12\n",
            "weighted avg       0.46      0.33      0.39        12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, math, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# --------- Reproducibility ----------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --------- Paths ----------\n",
        "DATA_PATH = \"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\"\n",
        "\n",
        "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"\n",
        "data = np.load(DATA_PATH, allow_pickle=True)\n",
        "\n",
        "X = data[\"X\"]          # (N, T=16, F=73)\n",
        "y = data[\"y\"]          # (N,)\n",
        "classes = data.get(\"classes\", None)\n",
        "\n",
        "# y may be str labels or ints depending on how you saved; handle both.\n",
        "if y.dtype.kind in (\"U\", \"S\", \"O\"):\n",
        "    # map string labels to ids in sorted order (or use 'classes' if present)\n",
        "    if classes is None:\n",
        "        uniq = sorted(list(set([str(v) for v in y])))\n",
        "    else:\n",
        "        uniq = [str(c) for c in classes.tolist()] if hasattr(classes, \"tolist\") else [str(c) for c in classes]\n",
        "    label2id = {lab:i for i, lab in enumerate(uniq)}\n",
        "    y_ids = np.array([label2id[str(v)] for v in y], dtype=np.int64)\n",
        "    CLASS_NAMES = uniq\n",
        "else:\n",
        "    y_ids = y.astype(np.int64)\n",
        "    # if classes stored, use that; else build generic names\n",
        "    if classes is not None:\n",
        "        CLASS_NAMES = [str(c) for c in (classes.tolist() if hasattr(classes, \"tolist\") else classes)]\n",
        "    else:\n",
        "        K = int(y_ids.max()) + 1\n",
        "        CLASS_NAMES = [f\"class_{i}\" for i in range(K)]\n",
        "\n",
        "N, T, F = X.shape\n",
        "K = int(y_ids.max()) + 1\n",
        "\n",
        "print(\"X:\", X.shape, \"y:\", y_ids.shape, \"num_classes:\", K)\n",
        "print(\"Class names:\", CLASS_NAMES)\n",
        "print(\"Class counts:\", np.bincount(y_ids, minlength=K))\n",
        "\n",
        "# --------- Dataset ----------\n",
        "class PoseSeqDS(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --------- Model (small Transformer) ----------\n",
        "class PoseTransformer(nn.Module):\n",
        "    def __init__(self, feat_dim=73, d_model=128, nhead=4, num_layers=2, dropout=0.2, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Linear(feat_dim, d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=4*d_model,\n",
        "            dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # x: (B,T,F)\n",
        "        x = self.in_proj(x)        # (B,T,D)\n",
        "        x = self.encoder(x)        # (B,T,D)\n",
        "        x = x.mean(dim=1)          # global average pooling over time\n",
        "        return self.cls(x)\n",
        "\n",
        "# --------- Training / Eval ----------\n",
        "def make_class_weights(y_train, num_classes):\n",
        "    counts = np.bincount(y_train, minlength=num_classes).astype(np.float32)\n",
        "    # inverse freq weights (safe)\n",
        "    weights = counts.sum() / (num_classes * np.maximum(counts, 1.0))\n",
        "    return torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "\n",
        "def train_one_fold(X, y, tr_idx, va_idx, fold_id,\n",
        "                   epochs=60, batch_size=8, lr=3e-4, wd=1e-4, patience=10):\n",
        "    train_ds = PoseSeqDS(X[tr_idx], y[tr_idx])\n",
        "    val_ds   = PoseSeqDS(X[va_idx], y[va_idx])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = PoseTransformer(feat_dim=F, num_classes=K).to(device)\n",
        "\n",
        "    class_w = make_class_weights(y[tr_idx], K)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_w)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    best_val = -1.0\n",
        "    best_state = None\n",
        "    bad = 0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_preds, tr_tgts = [], []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            tr_preds.append(logits.argmax(1).detach().cpu().numpy())\n",
        "            tr_tgts.append(yb.detach().cpu().numpy())\n",
        "\n",
        "        tr_preds = np.concatenate(tr_preds)\n",
        "        tr_tgts  = np.concatenate(tr_tgts)\n",
        "        tr_acc = accuracy_score(tr_tgts, tr_preds)\n",
        "\n",
        "        model.eval()\n",
        "        va_preds, va_tgts = [], []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                logits = model(xb)\n",
        "                va_preds.append(logits.argmax(1).cpu().numpy())\n",
        "                va_tgts.append(yb.numpy())\n",
        "        va_preds = np.concatenate(va_preds)\n",
        "        va_tgts  = np.concatenate(va_tgts)\n",
        "        va_acc = accuracy_score(va_tgts, va_preds)\n",
        "\n",
        "        if va_acc > best_val:\n",
        "            best_val = va_acc\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "\n",
        "        if ep == 1 or ep % 10 == 0:\n",
        "            print(f\"[Fold {fold_id}] Epoch {ep:02d} | train acc={tr_acc:.3f} | val acc={va_acc:.3f} | best={best_val:.3f}\")\n",
        "\n",
        "        if bad >= patience:\n",
        "            break\n",
        "\n",
        "    # Load best and return fold metrics + preds\n",
        "    model.load_state_dict(best_state)\n",
        "    model.to(device).eval()\n",
        "\n",
        "    va_preds, va_tgts = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            va_preds.append(logits.argmax(1).cpu().numpy())\n",
        "            va_tgts.append(yb.numpy())\n",
        "    va_preds = np.concatenate(va_preds)\n",
        "    va_tgts  = np.concatenate(va_tgts)\n",
        "\n",
        "    return best_val, va_tgts, va_preds\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_accs = []\n",
        "all_true = np.zeros(N, dtype=np.int64)\n",
        "all_pred = np.zeros(N, dtype=np.int64)\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y_ids), start=1):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Fold {fold} | train={len(tr_idx)} val={len(va_idx)}\")\n",
        "    print(\"Train counts:\", np.bincount(y_ids[tr_idx], minlength=K))\n",
        "    print(\"Val   counts:\", np.bincount(y_ids[va_idx], minlength=K))\n",
        "\n",
        "    best_val, va_tgts, va_preds = train_one_fold(X, y_ids, tr_idx, va_idx, fold_id=fold)\n",
        "    fold_accs.append(best_val)\n",
        "\n",
        "    all_true[va_idx] = va_tgts\n",
        "    all_pred[va_idx] = va_preds\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Fold accuracies:\", [round(a, 4) for a in fold_accs])\n",
        "print(f\"Mean CV acc: {np.mean(fold_accs):.4f}  | Std: {np.std(fold_accs):.4f}\")\n",
        "\n",
        "print(\"\\nOverall (OOF) confusion matrix:\")\n",
        "cm = confusion_matrix(all_true, all_pred)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nOverall (OOF) classification report:\")\n",
        "print(classification_report(all_true, all_pred, target_names=CLASS_NAMES, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYB1FdPoVI2S",
        "outputId": "af1a140f-9dc5-4b2f-ad26-656e420c775d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "X: (77, 16, 73) y: (77,) num_classes: 4\n",
            "Class names: ['class_0', 'class_1', 'class_2', 'class_3']\n",
            "Class counts: [10 17 21 29]\n",
            "\n",
            "============================================================\n",
            "Fold 1 | train=61 val=16\n",
            "Train counts: [ 8 13 16 24]\n",
            "Val   counts: [2 4 5 5]\n",
            "[Fold 1] Epoch 01 | train acc=0.311 | val acc=0.250 | best=0.250\n",
            "[Fold 1] Epoch 10 | train acc=0.393 | val acc=0.438 | best=0.438\n",
            "[Fold 1] Epoch 20 | train acc=0.541 | val acc=0.312 | best=0.500\n",
            "\n",
            "============================================================\n",
            "Fold 2 | train=61 val=16\n",
            "Train counts: [ 8 13 17 23]\n",
            "Val   counts: [2 4 4 6]\n",
            "[Fold 2] Epoch 01 | train acc=0.180 | val acc=0.312 | best=0.312\n",
            "[Fold 2] Epoch 10 | train acc=0.393 | val acc=0.312 | best=0.500\n",
            "\n",
            "============================================================\n",
            "Fold 3 | train=62 val=15\n",
            "Train counts: [ 8 14 17 23]\n",
            "Val   counts: [2 3 4 6]\n",
            "[Fold 3] Epoch 01 | train acc=0.194 | val acc=0.200 | best=0.200\n",
            "[Fold 3] Epoch 10 | train acc=0.355 | val acc=0.267 | best=0.267\n",
            "[Fold 3] Epoch 20 | train acc=0.468 | val acc=0.200 | best=0.333\n",
            "\n",
            "============================================================\n",
            "Fold 4 | train=62 val=15\n",
            "Train counts: [ 8 14 17 23]\n",
            "Val   counts: [2 3 4 6]\n",
            "[Fold 4] Epoch 01 | train acc=0.210 | val acc=0.067 | best=0.067\n",
            "[Fold 4] Epoch 10 | train acc=0.435 | val acc=0.333 | best=0.400\n",
            "\n",
            "============================================================\n",
            "Fold 5 | train=62 val=15\n",
            "Train counts: [ 8 14 17 23]\n",
            "Val   counts: [2 3 4 6]\n",
            "[Fold 5] Epoch 01 | train acc=0.226 | val acc=0.333 | best=0.333\n",
            "[Fold 5] Epoch 10 | train acc=0.403 | val acc=0.267 | best=0.400\n",
            "\n",
            "============================================================\n",
            "Fold accuracies: [0.5, 0.5, 0.3333, 0.4, 0.4]\n",
            "Mean CV acc: 0.4267  | Std: 0.0646\n",
            "\n",
            "Overall (OOF) confusion matrix:\n",
            "[[ 1  2  2  5]\n",
            " [ 2  0  3 12]\n",
            " [ 0  0 13  8]\n",
            " [ 3  3  4 19]]\n",
            "\n",
            "Overall (OOF) classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       0.17      0.10      0.12        10\n",
            "     class_1       0.00      0.00      0.00        17\n",
            "     class_2       0.59      0.62      0.60        21\n",
            "     class_3       0.43      0.66      0.52        29\n",
            "\n",
            "    accuracy                           0.43        77\n",
            "   macro avg       0.30      0.34      0.31        77\n",
            "weighted avg       0.35      0.43      0.38        77\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# -------------------------\n",
        "# Repro\n",
        "# -------------------------\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_all(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "POSE_NPZ = Path(\"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\")\n",
        "FRAMES_ROOT = Path(\"/content/drive/MyDrive/synthetic_videos/frames_person\")\n",
        "SPLIT_CSV = Path(\"/content/drive/MyDrive/synthetic_videos/split_grouped_final.csv\")  # if you want mapping, optional\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/synthetic_videos\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_BEST = SAVE_DIR / \"fusion_best_cv.pt\"\n",
        "\n",
        "\n",
        "data = np.load(POSE_NPZ, allow_pickle=True)\n",
        "X_pose = data[\"X\"]      # (N, 16, 73)\n",
        "y = data[\"y\"]           # (N,)\n",
        "classes = data.get(\"classes\", None)\n",
        "\n",
        "N, T, D = X_pose.shape\n",
        "y = y.astype(int)\n",
        "\n",
        "print(\"Pose X:\", X_pose.shape, \"y:\", y.shape)\n",
        "print(\"Num classes:\", len(np.unique(y)), \"Class counts:\", np.bincount(y))\n",
        "\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "def collect_video_dirs(frames_root: Path):\n",
        "\n",
        "    dirs = []\n",
        "    for d in frames_root.rglob(\"*\"):\n",
        "        if d.is_dir():\n",
        "            jpgs = list(d.glob(\"*.jpg\"))\n",
        "            if len(jpgs) > 0:\n",
        "                dirs.append(d)\n",
        "    return dirs\n",
        "\n",
        "video_dirs = collect_video_dirs(FRAMES_ROOT)\n",
        "\n",
        "stem_to_dir = {}\n",
        "for d in video_dirs:\n",
        "    stem = d.name\n",
        "    if stem not in stem_to_dir:\n",
        "        stem_to_dir[stem] = d\n",
        "\n",
        "stems = None\n",
        "try:\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(SPLIT_CSV)\n",
        "    if \"video_path\" in df.columns:\n",
        "        stems = [Path(p).stem for p in df[\"video_path\"].tolist()]\n",
        "        print(\"Loaded stems from split CSV:\", len(stems))\n",
        "except Exception as e:\n",
        "    stems = None\n",
        "\n",
        "if stems is None:\n",
        "    # fallback (may mismatch if order differs)\n",
        "    all_stems = sorted(stem_to_dir.keys())\n",
        "    if len(all_stems) < N:\n",
        "        raise RuntimeError(f\"Not enough stems in frames_person. Found {len(all_stems)} but need {N}.\")\n",
        "    stems = all_stems[:N]\n",
        "    print(\"WARNING: Using fallback stems list (sorted). This may mismatch with pose rows.\")\n",
        "\n",
        "# sanity: ensure each stem exists in frames_person\n",
        "missing = [s for s in stems if s not in stem_to_dir]\n",
        "print(\"Missing stems in frames_person:\", len(missing))\n",
        "if len(missing) > 0:\n",
        "    print(\"Example missing:\", missing[:5])\n",
        "\n",
        "# -------------------------\n",
        "# Appearance feature extractor (ResNet18 -> 512D)\n",
        "# -------------------------\n",
        "resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "resnet.fc = nn.Identity()\n",
        "resnet = resnet.to(device).eval()\n",
        "\n",
        "img_tfm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_app_feature_for_stem(stem: str, num_frames=16):\n",
        "    \"\"\"\n",
        "    returns (512,) float32\n",
        "    \"\"\"\n",
        "    if stem not in stem_to_dir:\n",
        "        return np.zeros((512,), dtype=np.float32)\n",
        "\n",
        "    d = stem_to_dir[stem]\n",
        "    imgs = sorted(d.glob(\"*.jpg\"))\n",
        "    if len(imgs) == 0:\n",
        "        return np.zeros((512,), dtype=np.float32)\n",
        "\n",
        "    # pick evenly spaced frames up to num_frames\n",
        "    if len(imgs) >= num_frames:\n",
        "        idxs = np.linspace(0, len(imgs)-1, num_frames).astype(int).tolist()\n",
        "        imgs = [imgs[i] for i in idxs]\n",
        "    else:\n",
        "        # pad by repeating last\n",
        "        while len(imgs) < num_frames:\n",
        "            imgs.append(imgs[-1])\n",
        "\n",
        "    feats = []\n",
        "    for p in imgs:\n",
        "        try:\n",
        "            im = Image.open(p).convert(\"RGB\")\n",
        "            x = img_tfm(im).unsqueeze(0).to(device)\n",
        "            f = resnet(x)          # (1, 512)\n",
        "            feats.append(f.squeeze(0).detach().cpu().numpy())\n",
        "        except:\n",
        "            # if image fails\n",
        "            feats.append(np.zeros((512,), dtype=np.float32))\n",
        "\n",
        "    feat = np.mean(np.stack(feats, axis=0), axis=0)\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "print(\"Extracting appearance features for all videos (may take some time)...\")\n",
        "X_app = np.stack([extract_app_feature_for_stem(s, num_frames=16) for s in stems], axis=0)  # (N, 512)\n",
        "print(\"Appearance X_app:\", X_app.shape)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class FusionDS(Dataset):\n",
        "    def __init__(self, X_pose, X_app, y, idxs):\n",
        "        self.X_pose = torch.tensor(X_pose[idxs], dtype=torch.float32)\n",
        "        self.X_app  = torch.tensor(X_app[idxs], dtype=torch.float32)\n",
        "        self.y      = torch.tensor(y[idxs], dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        return self.X_pose[i], self.X_app[i], self.y[i]\n",
        "\n",
        "class PoseTransformer(nn.Module):\n",
        "    def __init__(self, in_dim=73, d_model=128, nhead=4, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=256,\n",
        "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):  # x: (B,T,D)\n",
        "        x = self.proj(x)    # (B,T,dm)\n",
        "        x = self.enc(x)     # (B,T,dm)\n",
        "        # pool over T\n",
        "        x = x.transpose(1,2)        # (B,dm,T)\n",
        "        x = self.pool(x).squeeze(-1) # (B,dm)\n",
        "        return x\n",
        "\n",
        "class AppMLP(nn.Module):\n",
        "    def __init__(self, in_dim=512, hid=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hid, hid),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.pose = PoseTransformer(in_dim=73, d_model=128, nhead=4, num_layers=2, dropout=0.1)\n",
        "        self.app  = AppMLP(in_dim=512, hid=256, dropout=0.2)\n",
        "        self.cls  = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128+256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_pose, x_app):\n",
        "        p = self.pose(x_pose)\n",
        "        a = self.app(x_app)\n",
        "        z = torch.cat([p,a], dim=1)\n",
        "        return self.cls(z)\n",
        "\n",
        "\n",
        "def accuracy_from_logits(logits, y):\n",
        "    return (logits.argmax(1) == y).float().mean().item()\n",
        "\n",
        "def train_one_fold(model, train_loader, val_loader, class_weights, epochs=25, lr=3e-4, wd=1e-4):\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "\n",
        "    ce = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    best_val = -1\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_correct, tr_total = 0, 0\n",
        "        for xb_pose, xb_app, yb in train_loader:\n",
        "            xb_pose, xb_app, yb = xb_pose.to(device), xb_app.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb_pose, xb_app)\n",
        "            loss = ce(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            tr_correct += (logits.argmax(1) == yb).sum().item()\n",
        "            tr_total += yb.size(0)\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "        # val\n",
        "        model.eval()\n",
        "        va_correct, va_total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb_pose, xb_app, yb in val_loader:\n",
        "                xb_pose, xb_app, yb = xb_pose.to(device), xb_app.to(device), yb.to(device)\n",
        "                logits = model(xb_pose, xb_app)\n",
        "                va_correct += (logits.argmax(1) == yb).sum().item()\n",
        "                va_total += yb.size(0)\n",
        "\n",
        "        tr_acc = tr_correct / max(1, tr_total)\n",
        "        va_acc = va_correct / max(1, va_total)\n",
        "\n",
        "        if ep == 1 or ep % 5 == 0:\n",
        "            print(f\"Epoch {ep:02d} | train acc={tr_acc:.3f} | val acc={va_acc:.3f} | best={best_val if best_val>=0 else va_acc:.3f}\")\n",
        "\n",
        "        if va_acc > best_val:\n",
        "            best_val = va_acc\n",
        "            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model, best_val\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_loader(model, loader):\n",
        "    model.eval()\n",
        "    all_p, all_y = [], []\n",
        "    for xb_pose, xb_app, yb in loader:\n",
        "        xb_pose, xb_app = xb_pose.to(device), xb_app.to(device)\n",
        "        logits = model(xb_pose, xb_app)\n",
        "        all_p.append(logits.argmax(1).cpu().numpy())\n",
        "        all_y.append(yb.numpy())\n",
        "    return np.concatenate(all_p), np.concatenate(all_y)\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_accs = []\n",
        "oof_preds = np.zeros((N,), dtype=int)\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(N), y), 1):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Fold {fold} | train={len(tr_idx)} val={len(va_idx)}\")\n",
        "\n",
        "    # class weights from TRAIN only\n",
        "    tr_counts = np.bincount(y[tr_idx], minlength=num_classes).astype(np.float32)\n",
        "    tr_counts[tr_counts == 0] = 1.0\n",
        "    weights = (tr_counts.sum() / tr_counts)  # inverse freq\n",
        "    weights = weights / weights.mean()\n",
        "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "    print(\"Train counts:\", tr_counts.astype(int).tolist())\n",
        "    print(\"Class weights:\", np.round(weights, 3).tolist())\n",
        "\n",
        "    train_ds = FusionDS(X_pose, X_app, y, tr_idx)\n",
        "    val_ds   = FusionDS(X_pose, X_app, y, va_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = FusionModel(num_classes=num_classes)\n",
        "    model, best_val = train_one_fold(model, train_loader, val_loader, class_weights, epochs=25, lr=3e-4, wd=1e-4)\n",
        "\n",
        "    # val predictions\n",
        "    preds, targets = predict_loader(model, val_loader)\n",
        "    acc = (preds == targets).mean()\n",
        "    fold_accs.append(acc)\n",
        "    oof_preds[va_idx] = preds\n",
        "\n",
        "    print(f\"[Fold {fold}] Best val acc (during training): {best_val:.4f}\")\n",
        "    print(f\"[Fold {fold}] Final val acc (after best reload): {acc:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Fold accuracies:\", [round(a,4) for a in fold_accs])\n",
        "print(f\"Mean CV acc: {np.mean(fold_accs):.4f} | Std: {np.std(fold_accs):.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y, oof_preds)\n",
        "print(\"\\nOverall (OOF) confusion matrix:\\n\", cm)\n",
        "\n",
        "print(\"\\nOverall (OOF) classification report:\\n\")\n",
        "print(classification_report(y, oof_preds))\n",
        "\n",
        "full_counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
        "full_counts[full_counts == 0] = 1.0\n",
        "full_w = (full_counts.sum() / full_counts)\n",
        "full_w = full_w / full_w.mean()\n",
        "full_w = torch.tensor(full_w, dtype=torch.float32)\n",
        "\n",
        "full_idx = np.arange(N)\n",
        "full_ds = FusionDS(X_pose, X_app, y, full_idx)\n",
        "full_loader = DataLoader(full_ds, batch_size=16, shuffle=True, num_workers=0)\n",
        "\n",
        "val_size = max(8, N//5)\n",
        "val_idx = np.random.choice(full_idx, size=val_size, replace=False)\n",
        "tr_idx = np.array([i for i in full_idx if i not in set(val_idx)])\n",
        "BS = 16\n",
        "train_ds = FusionDS(X_pose, X_app, y, tr_idx)\n",
        "val_ds   = FusionDS(X_pose, X_app, y, val_idx)\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=BS, shuffle=False, drop_last=False)\n",
        "\n",
        "final_model = FusionModel(num_classes=num_classes)\n",
        "final_model, best_val = train_one_fold(final_model, train_loader, val_loader, full_w, epochs=35, lr=3e-4, wd=1e-4)\n",
        "\n",
        "torch.save(final_model.state_dict(), OUT_BEST)\n",
        "print(\"\\nSaved final fusion model to:\", OUT_BEST)\n",
        "print(\"Best val (monitor split):\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY4DjVZUYzXk",
        "outputId": "39cb0039-bf24-45a7-d6ac-07715d1cc830"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Pose X: (77, 16, 73) y: (77,)\n",
            "Num classes: 4 Class counts: [10 17 21 29]\n",
            "Loaded stems from split CSV: 77\n",
            "Missing stems in frames_person: 0\n",
            "Extracting appearance features for all videos (may take some time)...\n",
            "Appearance X_app: (77, 512)\n",
            "\n",
            "============================================================\n",
            "Fold 1 | train=61 val=16\n",
            "Train counts: [8, 13, 16, 24]\n",
            "Class weights: [1.6339999437332153, 1.0049999952316284, 0.8169999718666077, 0.5450000166893005]\n",
            "Epoch 01 | train acc=0.262 | val acc=0.375 | best=0.375\n",
            "Epoch 05 | train acc=0.361 | val acc=0.250 | best=0.375\n",
            "Epoch 10 | train acc=0.344 | val acc=0.312 | best=0.375\n",
            "Epoch 15 | train acc=0.459 | val acc=0.250 | best=0.375\n",
            "Epoch 20 | train acc=0.443 | val acc=0.375 | best=0.375\n",
            "Epoch 25 | train acc=0.475 | val acc=0.375 | best=0.375\n",
            "[Fold 1] Best val acc (during training): 0.3750\n",
            "[Fold 1] Final val acc (after best reload): 0.3750\n",
            "\n",
            "============================================================\n",
            "Fold 2 | train=61 val=16\n",
            "Train counts: [8, 13, 17, 23]\n",
            "Class weights: [1.6440000534057617, 1.0110000371932983, 0.7730000019073486, 0.5720000267028809]\n",
            "Epoch 01 | train acc=0.115 | val acc=0.250 | best=0.250\n",
            "Epoch 05 | train acc=0.377 | val acc=0.438 | best=0.375\n",
            "Epoch 10 | train acc=0.459 | val acc=0.312 | best=0.438\n",
            "Epoch 15 | train acc=0.508 | val acc=0.500 | best=0.438\n",
            "Epoch 20 | train acc=0.541 | val acc=0.250 | best=0.500\n",
            "Epoch 25 | train acc=0.492 | val acc=0.250 | best=0.500\n",
            "[Fold 2] Best val acc (during training): 0.5000\n",
            "[Fold 2] Final val acc (after best reload): 0.5000\n",
            "\n",
            "============================================================\n",
            "Fold 3 | train=62 val=15\n",
            "Train counts: [8, 14, 17, 23]\n",
            "Class weights: [1.6740000247955322, 0.9559999704360962, 0.7879999876022339, 0.5820000171661377]\n",
            "Epoch 01 | train acc=0.242 | val acc=0.333 | best=0.333\n",
            "Epoch 05 | train acc=0.484 | val acc=0.133 | best=0.333\n",
            "Epoch 10 | train acc=0.548 | val acc=0.133 | best=0.333\n",
            "Epoch 15 | train acc=0.548 | val acc=0.133 | best=0.333\n",
            "Epoch 20 | train acc=0.613 | val acc=0.200 | best=0.333\n",
            "Epoch 25 | train acc=0.581 | val acc=0.200 | best=0.333\n",
            "[Fold 3] Best val acc (during training): 0.3333\n",
            "[Fold 3] Final val acc (after best reload): 0.3333\n",
            "\n",
            "============================================================\n",
            "Fold 4 | train=62 val=15\n",
            "Train counts: [8, 14, 17, 23]\n",
            "Class weights: [1.6740000247955322, 0.9559999704360962, 0.7879999876022339, 0.5820000171661377]\n",
            "Epoch 01 | train acc=0.274 | val acc=0.267 | best=0.267\n",
            "Epoch 05 | train acc=0.371 | val acc=0.267 | best=0.333\n",
            "Epoch 10 | train acc=0.468 | val acc=0.333 | best=0.400\n",
            "Epoch 15 | train acc=0.500 | val acc=0.267 | best=0.400\n",
            "Epoch 20 | train acc=0.452 | val acc=0.333 | best=0.400\n",
            "Epoch 25 | train acc=0.500 | val acc=0.333 | best=0.400\n",
            "[Fold 4] Best val acc (during training): 0.4000\n",
            "[Fold 4] Final val acc (after best reload): 0.4000\n",
            "\n",
            "============================================================\n",
            "Fold 5 | train=62 val=15\n",
            "Train counts: [8, 14, 17, 23]\n",
            "Class weights: [1.6740000247955322, 0.9559999704360962, 0.7879999876022339, 0.5820000171661377]\n",
            "Epoch 01 | train acc=0.226 | val acc=0.133 | best=0.133\n",
            "Epoch 05 | train acc=0.468 | val acc=0.333 | best=0.333\n",
            "Epoch 10 | train acc=0.371 | val acc=0.267 | best=0.400\n",
            "Epoch 15 | train acc=0.565 | val acc=0.333 | best=0.400\n",
            "Epoch 20 | train acc=0.629 | val acc=0.200 | best=0.400\n",
            "Epoch 25 | train acc=0.629 | val acc=0.200 | best=0.400\n",
            "[Fold 5] Best val acc (during training): 0.4000\n",
            "[Fold 5] Final val acc (after best reload): 0.4000\n",
            "\n",
            "============================================================\n",
            "Fold accuracies: [np.float64(0.375), np.float64(0.5), np.float64(0.3333), np.float64(0.4), np.float64(0.4)]\n",
            "Mean CV acc: 0.4017 | Std: 0.0549\n",
            "\n",
            "Overall (OOF) confusion matrix:\n",
            " [[ 1  1  5  3]\n",
            " [ 3  4  5  5]\n",
            " [ 0  2 18  1]\n",
            " [ 5  9  7  8]]\n",
            "\n",
            "Overall (OOF) classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.10      0.11        10\n",
            "           1       0.25      0.24      0.24        17\n",
            "           2       0.51      0.86      0.64        21\n",
            "           3       0.47      0.28      0.35        29\n",
            "\n",
            "    accuracy                           0.40        77\n",
            "   macro avg       0.34      0.37      0.33        77\n",
            "weighted avg       0.39      0.40      0.37        77\n",
            "\n",
            "Epoch 01 | train acc=0.113 | val acc=0.267 | best=0.267\n",
            "Epoch 05 | train acc=0.371 | val acc=0.267 | best=0.333\n",
            "Epoch 10 | train acc=0.387 | val acc=0.267 | best=0.333\n",
            "Epoch 15 | train acc=0.468 | val acc=0.067 | best=0.400\n",
            "Epoch 20 | train acc=0.532 | val acc=0.200 | best=0.400\n",
            "Epoch 25 | train acc=0.532 | val acc=0.133 | best=0.400\n",
            "Epoch 30 | train acc=0.532 | val acc=0.267 | best=0.400\n",
            "Epoch 35 | train acc=0.532 | val acc=0.267 | best=0.400\n",
            "\n",
            "Saved final fusion model to: /content/drive/MyDrive/synthetic_videos/fusion_best_cv.pt\n",
            "Best val (monitor split): 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\"\n",
        "\n",
        "pose = np.load(DATA_PATH, allow_pickle=True)\n",
        "print(\"Keys:\", pose.files)\n",
        "\n",
        "X_pose = pose[\"X\"] if \"X\" in pose.files else pose[\"pose\"]\n",
        "y = pose[\"y\"] if \"y\" in pose.files else pose[\"labels\"]\n",
        "\n",
        "print(\"X_pose:\", X_pose.shape, X_pose.dtype)\n",
        "print(\"y:\", y.shape, y.dtype)\n",
        "print(\"Classes:\", np.unique(y), \"count:\", len(np.unique(y)))\n",
        "print(\"class_names:\", pose[\"class_names\"] if \"class_names\" in pose.files else None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjYvJJjffM_g",
        "outputId": "d8488afb-a9ed-4aca-eee9-bd601484dae3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: ['X', 'y', 'splits', 'class_names']\n",
            "X_pose: (77, 16, 73) float32\n",
            "y: (77,) int64\n",
            "Classes: [0 1 2 3] count: 4\n",
            "class_names: ['emotion' 'social' 'physical' 'pose_idle']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/synthetic_videos/pose_features_16f.npz\"\n",
        "data = np.load(DATA_PATH, allow_pickle=True)\n",
        "\n",
        "X = data[\"X\"]          # (77,16,73)\n",
        "y = data[\"y\"]          # (77,)\n",
        "class_names = list(data[\"class_names\"])\n",
        "\n",
        "# split\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, test_idx = next(sss.split(X, y))\n",
        "\n",
        "Xtr, ytr = X[train_idx], y[train_idx]\n",
        "Xte, yte = X[test_idx], y[test_idx]\n",
        "\n",
        "print(\"Train:\", Xtr.shape, \"Test:\", Xte.shape)\n",
        "print(\"Class names:\", class_names)\n",
        "print(\"Test class counts:\", np.unique(yte, return_counts=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEZe8ObJfNrn",
        "outputId": "e0104c10-5027-4aff-8b1e-2080d655a6d1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (61, 16, 73) Test: (16, 16, 73)\n",
            "Class names: [np.str_('emotion'), np.str_('social'), np.str_('physical'), np.str_('pose_idle')]\n",
            "Test class counts: (array([0, 1, 2, 3]), array([2, 4, 4, 6]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"len(yte)  =\", len(yte))\n",
        "print(\"len(preds)=\", len(preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fka68lMLvvq9",
        "outputId": "038c928a-d422-4e03-de50-be49d2025026"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(yte)  = 16\n",
            "len(preds)= 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_pose, x_app, yb in val_loader:\n",
        "        x_pose = x_pose.to(device)\n",
        "        x_app  = x_app.to(device)\n",
        "\n",
        "        logits = model(x_pose, x_app)          # FusionModel\n",
        "        pred = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        all_preds.append(pred)\n",
        "        all_true.append(yb.cpu().numpy())\n",
        "\n",
        "preds = np.concatenate(all_preds)\n",
        "y_eval = np.concatenate(all_true)\n",
        "\n",
        "print(\"len(y_eval) =\", len(y_eval), \"len(preds) =\", len(preds))\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_eval, preds))\n",
        "print(\"Macro-F1:\", f1_score(y_eval, preds, average=\"macro\"))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_eval, preds))\n",
        "print(\"\\nReport:\\n\", classification_report(y_eval, preds, target_names=class_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7OYyRE9vk4b",
        "outputId": "2a5819de-962e-47f4-d104-3ae9939d0923"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(y_eval) = 15 len(preds) = 15\n",
            "Accuracy: 0.4\n",
            "Macro-F1: 0.31060606060606055\n",
            "Confusion matrix:\n",
            " [[0 0 0 1]\n",
            " [1 1 1 1]\n",
            " [0 0 3 1]\n",
            " [0 1 3 2]]\n",
            "\n",
            "Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     emotion       0.00      0.00      0.00         1\n",
            "      social       0.50      0.25      0.33         4\n",
            "    physical       0.43      0.75      0.55         4\n",
            "   pose_idle       0.40      0.33      0.36         6\n",
            "\n",
            "    accuracy                           0.40        15\n",
            "   macro avg       0.33      0.33      0.31        15\n",
            "weighted avg       0.41      0.40      0.38        15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "os.makedirs(\"/content/drive/MyDrive/synthetic_videos\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/synthetic_videos/fusion_best.pt\")\n",
        "print(\"Saved:\", \"/content/drive/MyDrive/synthetic_videos/fusion_best.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q880uW0uvntT",
        "outputId": "a14ee831-6f5d-4adb-eb95-8dee32c5eece"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/synthetic_videos/fusion_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"\"\"\\\n",
        "Synthetic Video Project — Final Summary (Fusion Model)\n",
        "\n",
        "Dataset:\n",
        "- Source folder: /content/drive/MyDrive/synthetic_videos\n",
        "- pose_features_16f.npz: X (77, 16, 73), y (77), classes: ['emotion','social','physical','pose_idle']\n",
        "- Appearance features: 512-d per sample (used in fusion)\n",
        "\n",
        "Model:\n",
        "- FusionModel = Pose Transformer stream + Appearance MLP stream → fused classifier\n",
        "- Num classes: 4\n",
        "\n",
        "Final Validation:\n",
        "- N_val: 15\n",
        "- Accuracy: 0.40\n",
        "- Macro-F1: 0.3106\n",
        "\n",
        "Confusion Matrix:\n",
        "[[0 0 0 1]\n",
        " [1 1 1 1]\n",
        " [0 0 3 1]\n",
        " [0 1 3 2]]\n",
        "\n",
        "Saved model:\n",
        "- /content/drive/MyDrive/synthetic_videos/fusion_best.pt\n",
        "\"\"\"\n",
        "\n",
        "out_path = \"/content/drive/MyDrive/synthetic_videos/RESULTS_SUMMARY.txt\"\n",
        "\n",
        "with open(out_path, \"w\") as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"Saved:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn4YGOpHzJOZ",
        "outputId": "89bf00b9-5bf0-440f-e588-1502971b9e74"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/synthetic_videos/RESULTS_SUMMARY.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/drive/MyDrive/synthetic_videos/RESULTS_SUMMARY.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCS33e6L0NCk",
        "outputId": "64e5e435-030a-4733-8f3e-989bee0c92b9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 595 Jan 14 18:44 /content/drive/MyDrive/synthetic_videos/RESULTS_SUMMARY.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUgC26xh0qBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}